{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c9e0b364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt # Import for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a24117ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 1 # Bending stiffness is the only input\n",
    "OUTPUT_SIZE = 3 # Thickness, Height, Angle are outputs\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 32 # \n",
    "NUM_EPOCHS = 1000\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1dd7ce00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 685 duplicate row(s) from the dataset.\n",
      "Shape of dataset after removing duplicates: (743, 4)\n",
      "YTest headers:\n",
      "[[  7.236  51.739  46.9  ]\n",
      " [  6.713 117.335  69.5  ]\n",
      " [  8.18   93.69   64.9  ]\n",
      " [  5.571  24.436  64.9  ]\n",
      " [  4.427  26.159  50.9  ]]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\mason\\Work\\CMEC_SandwichPanel\\Models\\MLP\\processed_bending_stiffness.csv')\n",
    "\n",
    "# Remove duplicates\n",
    "initial_count = len(df)\n",
    "df = df.drop_duplicates()\n",
    "removed_count = initial_count - len(df)\n",
    "if removed_count > 0:\n",
    "    print(f\"Removed {removed_count} duplicate row(s) from the dataset.\")\n",
    "print(f\"Shape of dataset after removing duplicates: {df.shape}\")\n",
    "\n",
    "X = df[['Bending_Stiffness']]\n",
    "y = df[['Thickness', 'Height', 'Angle (deg)']] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X.values, \n",
    "    y.values, \n",
    "    test_size=TEST_SIZE, \n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "print(f\"YTest headers:\\n{y_test[:5]}\")\n",
    "DATASET_SIZE = len(df) # Number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa038e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "        nn.Linear(INPUT_SIZE, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.2),\n",
    "        \n",
    "        nn.Linear(64, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.2),\n",
    "\n",
    "        nn.Linear(32, 16),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.2),\n",
    "        \n",
    "        # Output Layer\n",
    "        nn.Linear(16, OUTPUT_SIZE)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d0b235b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.from_numpy(X_train.astype(np.float32))\n",
    "y_train_tensor = torch.from_numpy(y_train.astype(np.float32))\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Convert TESTING data to PyTorch Tensors\n",
    "X_test_tensor = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_test_tensor = torch.from_numpy(y_test.astype(np.float32))\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c8f39df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training ---\n",
      "Epoch [1/1000], Train Loss (t_loss): 36958.2849, Val Loss (v_loss): 3546.9052\n",
      "Epoch [2/1000], Train Loss (t_loss): 8590.0279, Val Loss (v_loss): 3224.9978\n",
      "Epoch [3/1000], Train Loss (t_loss): 4253.9321, Val Loss (v_loss): 3018.6214\n",
      "Epoch [4/1000], Train Loss (t_loss): 2922.7666, Val Loss (v_loss): 2626.7519\n",
      "Epoch [5/1000], Train Loss (t_loss): 2536.2053, Val Loss (v_loss): 2156.3438\n",
      "Epoch [6/1000], Train Loss (t_loss): 2481.0425, Val Loss (v_loss): 2195.5674\n",
      "Epoch [7/1000], Train Loss (t_loss): 2365.6939, Val Loss (v_loss): 2572.0379\n",
      "Epoch [8/1000], Train Loss (t_loss): 2353.4084, Val Loss (v_loss): 2564.1617\n",
      "Epoch [9/1000], Train Loss (t_loss): 2375.5307, Val Loss (v_loss): 2380.5770\n",
      "Epoch [10/1000], Train Loss (t_loss): 2227.5819, Val Loss (v_loss): 2219.0192\n",
      "Epoch [11/1000], Train Loss (t_loss): 2060.7530, Val Loss (v_loss): 1929.4354\n",
      "Epoch [12/1000], Train Loss (t_loss): 1824.6842, Val Loss (v_loss): 1547.4236\n",
      "Epoch [13/1000], Train Loss (t_loss): 1899.7860, Val Loss (v_loss): 1656.5218\n",
      "Epoch [14/1000], Train Loss (t_loss): 1706.2265, Val Loss (v_loss): 1793.0416\n",
      "Epoch [15/1000], Train Loss (t_loss): 1892.9861, Val Loss (v_loss): 1458.2855\n",
      "Epoch [16/1000], Train Loss (t_loss): 1654.1184, Val Loss (v_loss): 1365.8163\n",
      "Epoch [17/1000], Train Loss (t_loss): 1545.1475, Val Loss (v_loss): 1419.9062\n",
      "Epoch [18/1000], Train Loss (t_loss): 1523.3457, Val Loss (v_loss): 1282.7236\n",
      "Epoch [19/1000], Train Loss (t_loss): 1503.7424, Val Loss (v_loss): 1271.5716\n",
      "Epoch [20/1000], Train Loss (t_loss): 1450.4256, Val Loss (v_loss): 1242.3782\n",
      "Epoch [21/1000], Train Loss (t_loss): 1294.4179, Val Loss (v_loss): 1027.4938\n",
      "Epoch [22/1000], Train Loss (t_loss): 1365.6304, Val Loss (v_loss): 1239.2124\n",
      "Epoch [23/1000], Train Loss (t_loss): 1279.0613, Val Loss (v_loss): 1150.5040\n",
      "Epoch [24/1000], Train Loss (t_loss): 1314.1787, Val Loss (v_loss): 1061.2449\n",
      "Epoch [25/1000], Train Loss (t_loss): 1340.8750, Val Loss (v_loss): 976.6918\n",
      "Epoch [26/1000], Train Loss (t_loss): 1337.1130, Val Loss (v_loss): 1129.0515\n",
      "Epoch [27/1000], Train Loss (t_loss): 1277.7367, Val Loss (v_loss): 1055.9958\n",
      "Epoch [28/1000], Train Loss (t_loss): 1243.2635, Val Loss (v_loss): 1194.6034\n",
      "Epoch [29/1000], Train Loss (t_loss): 1144.7053, Val Loss (v_loss): 873.3032\n",
      "Epoch [30/1000], Train Loss (t_loss): 1245.6364, Val Loss (v_loss): 1176.5058\n",
      "Epoch [31/1000], Train Loss (t_loss): 1160.6040, Val Loss (v_loss): 887.9850\n",
      "Epoch [32/1000], Train Loss (t_loss): 1196.8220, Val Loss (v_loss): 1013.5752\n",
      "Epoch [33/1000], Train Loss (t_loss): 1224.0157, Val Loss (v_loss): 1011.7158\n",
      "Epoch [34/1000], Train Loss (t_loss): 1150.6362, Val Loss (v_loss): 889.4388\n",
      "Epoch [35/1000], Train Loss (t_loss): 1092.2829, Val Loss (v_loss): 1117.9146\n",
      "Epoch [36/1000], Train Loss (t_loss): 1031.6367, Val Loss (v_loss): 906.5745\n",
      "Epoch [37/1000], Train Loss (t_loss): 1103.4400, Val Loss (v_loss): 1021.0842\n",
      "Epoch [38/1000], Train Loss (t_loss): 1021.4653, Val Loss (v_loss): 1002.5442\n",
      "Epoch [39/1000], Train Loss (t_loss): 1138.0578, Val Loss (v_loss): 912.2464\n",
      "Epoch [40/1000], Train Loss (t_loss): 1078.5746, Val Loss (v_loss): 1023.6867\n",
      "Epoch [41/1000], Train Loss (t_loss): 1139.0632, Val Loss (v_loss): 999.7048\n",
      "Epoch [42/1000], Train Loss (t_loss): 983.9361, Val Loss (v_loss): 801.9834\n",
      "Epoch [43/1000], Train Loss (t_loss): 1031.0297, Val Loss (v_loss): 961.5238\n",
      "Epoch [44/1000], Train Loss (t_loss): 1000.5903, Val Loss (v_loss): 888.4209\n",
      "Epoch [45/1000], Train Loss (t_loss): 966.8309, Val Loss (v_loss): 838.0103\n",
      "Epoch [46/1000], Train Loss (t_loss): 1077.5832, Val Loss (v_loss): 847.5873\n",
      "Epoch [47/1000], Train Loss (t_loss): 1090.7391, Val Loss (v_loss): 929.7721\n",
      "Epoch [48/1000], Train Loss (t_loss): 1040.5064, Val Loss (v_loss): 854.0705\n",
      "Epoch [49/1000], Train Loss (t_loss): 1017.6716, Val Loss (v_loss): 940.9698\n",
      "Epoch [50/1000], Train Loss (t_loss): 996.5220, Val Loss (v_loss): 914.4590\n",
      "Epoch [51/1000], Train Loss (t_loss): 984.6329, Val Loss (v_loss): 845.6059\n",
      "Epoch [52/1000], Train Loss (t_loss): 996.4809, Val Loss (v_loss): 919.3859\n",
      "Epoch [53/1000], Train Loss (t_loss): 974.9068, Val Loss (v_loss): 914.6461\n",
      "Epoch [54/1000], Train Loss (t_loss): 1006.8528, Val Loss (v_loss): 835.1797\n",
      "Epoch [55/1000], Train Loss (t_loss): 968.1766, Val Loss (v_loss): 894.8080\n",
      "Epoch [56/1000], Train Loss (t_loss): 933.4358, Val Loss (v_loss): 914.7347\n",
      "Epoch [57/1000], Train Loss (t_loss): 961.4513, Val Loss (v_loss): 812.3719\n",
      "Epoch [58/1000], Train Loss (t_loss): 919.8746, Val Loss (v_loss): 967.4394\n",
      "Epoch [59/1000], Train Loss (t_loss): 1006.9062, Val Loss (v_loss): 839.5985\n",
      "Epoch [60/1000], Train Loss (t_loss): 938.4031, Val Loss (v_loss): 960.8222\n",
      "Epoch [61/1000], Train Loss (t_loss): 978.8667, Val Loss (v_loss): 888.6325\n",
      "Epoch [62/1000], Train Loss (t_loss): 1012.4411, Val Loss (v_loss): 975.4206\n",
      "Epoch [63/1000], Train Loss (t_loss): 1009.2882, Val Loss (v_loss): 749.2672\n",
      "Epoch [64/1000], Train Loss (t_loss): 951.1649, Val Loss (v_loss): 827.9225\n",
      "Epoch [65/1000], Train Loss (t_loss): 952.1608, Val Loss (v_loss): 1110.2002\n",
      "Epoch [66/1000], Train Loss (t_loss): 965.5075, Val Loss (v_loss): 812.5759\n",
      "Epoch [67/1000], Train Loss (t_loss): 980.9536, Val Loss (v_loss): 841.4035\n",
      "Epoch [68/1000], Train Loss (t_loss): 998.7212, Val Loss (v_loss): 1000.8988\n",
      "Epoch [69/1000], Train Loss (t_loss): 856.3669, Val Loss (v_loss): 761.1461\n",
      "Epoch [70/1000], Train Loss (t_loss): 960.2663, Val Loss (v_loss): 974.8274\n",
      "Epoch [71/1000], Train Loss (t_loss): 947.7281, Val Loss (v_loss): 799.7353\n",
      "Epoch [72/1000], Train Loss (t_loss): 900.4593, Val Loss (v_loss): 807.1341\n",
      "Epoch [73/1000], Train Loss (t_loss): 947.7362, Val Loss (v_loss): 965.6476\n",
      "Epoch [74/1000], Train Loss (t_loss): 980.6069, Val Loss (v_loss): 922.9237\n",
      "Epoch [75/1000], Train Loss (t_loss): 1048.6144, Val Loss (v_loss): 785.2295\n",
      "Epoch [76/1000], Train Loss (t_loss): 948.3136, Val Loss (v_loss): 784.3252\n",
      "Epoch [77/1000], Train Loss (t_loss): 986.4265, Val Loss (v_loss): 873.6573\n",
      "Epoch [78/1000], Train Loss (t_loss): 995.3852, Val Loss (v_loss): 815.3910\n",
      "Epoch [79/1000], Train Loss (t_loss): 862.3338, Val Loss (v_loss): 702.6060\n",
      "Epoch [80/1000], Train Loss (t_loss): 958.9845, Val Loss (v_loss): 980.0101\n",
      "Epoch [81/1000], Train Loss (t_loss): 952.6511, Val Loss (v_loss): 849.1765\n",
      "Epoch [82/1000], Train Loss (t_loss): 857.8629, Val Loss (v_loss): 738.3340\n",
      "Epoch [83/1000], Train Loss (t_loss): 949.3357, Val Loss (v_loss): 925.6755\n",
      "Epoch [84/1000], Train Loss (t_loss): 960.1046, Val Loss (v_loss): 871.9488\n",
      "Epoch [85/1000], Train Loss (t_loss): 901.5136, Val Loss (v_loss): 907.3254\n",
      "Epoch [86/1000], Train Loss (t_loss): 955.9803, Val Loss (v_loss): 885.0565\n",
      "Epoch [87/1000], Train Loss (t_loss): 954.5293, Val Loss (v_loss): 892.7879\n",
      "Epoch [88/1000], Train Loss (t_loss): 985.6395, Val Loss (v_loss): 797.2378\n",
      "Epoch [89/1000], Train Loss (t_loss): 883.3342, Val Loss (v_loss): 822.3875\n",
      "Epoch [90/1000], Train Loss (t_loss): 869.7518, Val Loss (v_loss): 884.8653\n",
      "Epoch [91/1000], Train Loss (t_loss): 928.6260, Val Loss (v_loss): 881.0518\n",
      "Epoch [92/1000], Train Loss (t_loss): 922.2454, Val Loss (v_loss): 880.0266\n",
      "Epoch [93/1000], Train Loss (t_loss): 968.0732, Val Loss (v_loss): 769.2707\n",
      "Epoch [94/1000], Train Loss (t_loss): 950.3493, Val Loss (v_loss): 712.2770\n",
      "Epoch [95/1000], Train Loss (t_loss): 868.3987, Val Loss (v_loss): 810.0541\n",
      "Epoch [96/1000], Train Loss (t_loss): 909.7279, Val Loss (v_loss): 851.4270\n",
      "Epoch [97/1000], Train Loss (t_loss): 937.1500, Val Loss (v_loss): 685.2163\n",
      "Epoch [98/1000], Train Loss (t_loss): 889.4230, Val Loss (v_loss): 896.4558\n",
      "Epoch [99/1000], Train Loss (t_loss): 932.5796, Val Loss (v_loss): 822.6043\n",
      "Epoch [100/1000], Train Loss (t_loss): 913.2240, Val Loss (v_loss): 812.5106\n",
      "Epoch [101/1000], Train Loss (t_loss): 944.2954, Val Loss (v_loss): 880.0151\n",
      "Epoch [102/1000], Train Loss (t_loss): 917.7108, Val Loss (v_loss): 737.2147\n",
      "Epoch [103/1000], Train Loss (t_loss): 890.5320, Val Loss (v_loss): 776.5012\n",
      "Epoch [104/1000], Train Loss (t_loss): 855.4149, Val Loss (v_loss): 764.4985\n",
      "Epoch [105/1000], Train Loss (t_loss): 882.7691, Val Loss (v_loss): 709.7414\n",
      "Epoch [106/1000], Train Loss (t_loss): 874.6322, Val Loss (v_loss): 834.1363\n",
      "Epoch [107/1000], Train Loss (t_loss): 776.5466, Val Loss (v_loss): 691.8976\n",
      "Epoch [108/1000], Train Loss (t_loss): 848.6738, Val Loss (v_loss): 796.8675\n",
      "Epoch [109/1000], Train Loss (t_loss): 906.7531, Val Loss (v_loss): 867.6906\n",
      "Epoch [110/1000], Train Loss (t_loss): 843.0896, Val Loss (v_loss): 758.7142\n",
      "Epoch [111/1000], Train Loss (t_loss): 904.6456, Val Loss (v_loss): 796.8316\n",
      "Epoch [112/1000], Train Loss (t_loss): 848.0141, Val Loss (v_loss): 716.8652\n",
      "Epoch [113/1000], Train Loss (t_loss): 985.4827, Val Loss (v_loss): 1015.9509\n",
      "Epoch [114/1000], Train Loss (t_loss): 818.2505, Val Loss (v_loss): 699.5992\n",
      "Epoch [115/1000], Train Loss (t_loss): 802.1316, Val Loss (v_loss): 780.2114\n",
      "Epoch [116/1000], Train Loss (t_loss): 945.7698, Val Loss (v_loss): 829.2646\n",
      "Epoch [117/1000], Train Loss (t_loss): 860.2779, Val Loss (v_loss): 657.4482\n",
      "Epoch [118/1000], Train Loss (t_loss): 908.6745, Val Loss (v_loss): 815.9579\n",
      "Epoch [119/1000], Train Loss (t_loss): 893.8090, Val Loss (v_loss): 743.2460\n",
      "Epoch [120/1000], Train Loss (t_loss): 867.0346, Val Loss (v_loss): 789.1797\n",
      "Epoch [121/1000], Train Loss (t_loss): 796.1271, Val Loss (v_loss): 702.4324\n",
      "Epoch [122/1000], Train Loss (t_loss): 872.9382, Val Loss (v_loss): 835.4091\n",
      "Epoch [123/1000], Train Loss (t_loss): 956.8973, Val Loss (v_loss): 928.4823\n",
      "Epoch [124/1000], Train Loss (t_loss): 906.7440, Val Loss (v_loss): 958.9815\n",
      "Epoch [125/1000], Train Loss (t_loss): 885.3319, Val Loss (v_loss): 793.4279\n",
      "Epoch [126/1000], Train Loss (t_loss): 857.2041, Val Loss (v_loss): 795.9582\n",
      "Epoch [127/1000], Train Loss (t_loss): 856.3615, Val Loss (v_loss): 845.9579\n",
      "Epoch [128/1000], Train Loss (t_loss): 878.9145, Val Loss (v_loss): 940.7118\n",
      "Epoch [129/1000], Train Loss (t_loss): 904.0156, Val Loss (v_loss): 929.7893\n",
      "Epoch [130/1000], Train Loss (t_loss): 913.5866, Val Loss (v_loss): 801.2805\n",
      "Epoch [131/1000], Train Loss (t_loss): 880.0004, Val Loss (v_loss): 791.9594\n",
      "Epoch [132/1000], Train Loss (t_loss): 785.5585, Val Loss (v_loss): 972.2978\n",
      "Epoch [133/1000], Train Loss (t_loss): 927.1451, Val Loss (v_loss): 783.6682\n",
      "Epoch [134/1000], Train Loss (t_loss): 825.6347, Val Loss (v_loss): 806.8043\n",
      "Epoch [135/1000], Train Loss (t_loss): 890.9583, Val Loss (v_loss): 861.5642\n",
      "Epoch [136/1000], Train Loss (t_loss): 828.3138, Val Loss (v_loss): 817.6214\n",
      "Epoch [137/1000], Train Loss (t_loss): 855.6586, Val Loss (v_loss): 760.4304\n",
      "Epoch [138/1000], Train Loss (t_loss): 809.0525, Val Loss (v_loss): 960.7858\n",
      "Epoch [139/1000], Train Loss (t_loss): 851.6940, Val Loss (v_loss): 731.9140\n",
      "Epoch [140/1000], Train Loss (t_loss): 844.1049, Val Loss (v_loss): 757.2879\n",
      "Epoch [141/1000], Train Loss (t_loss): 802.8619, Val Loss (v_loss): 795.7806\n",
      "Epoch [142/1000], Train Loss (t_loss): 890.6765, Val Loss (v_loss): 788.5070\n",
      "Epoch [143/1000], Train Loss (t_loss): 823.7923, Val Loss (v_loss): 866.6686\n",
      "Epoch [144/1000], Train Loss (t_loss): 805.9458, Val Loss (v_loss): 757.7296\n",
      "Epoch [145/1000], Train Loss (t_loss): 805.5170, Val Loss (v_loss): 787.1645\n",
      "Epoch [146/1000], Train Loss (t_loss): 801.7875, Val Loss (v_loss): 712.8246\n",
      "Epoch [147/1000], Train Loss (t_loss): 868.0030, Val Loss (v_loss): 740.6618\n",
      "Epoch [148/1000], Train Loss (t_loss): 848.8565, Val Loss (v_loss): 808.9836\n",
      "Epoch [149/1000], Train Loss (t_loss): 794.9694, Val Loss (v_loss): 706.0965\n",
      "Epoch [150/1000], Train Loss (t_loss): 776.7636, Val Loss (v_loss): 863.3327\n",
      "Epoch [151/1000], Train Loss (t_loss): 760.4018, Val Loss (v_loss): 846.0487\n",
      "Epoch [152/1000], Train Loss (t_loss): 803.4919, Val Loss (v_loss): 733.7670\n",
      "Epoch [153/1000], Train Loss (t_loss): 795.0111, Val Loss (v_loss): 883.0822\n",
      "Epoch [154/1000], Train Loss (t_loss): 790.6537, Val Loss (v_loss): 765.3458\n",
      "Epoch [155/1000], Train Loss (t_loss): 838.1170, Val Loss (v_loss): 757.3742\n",
      "Epoch [156/1000], Train Loss (t_loss): 799.7358, Val Loss (v_loss): 650.5691\n",
      "Epoch [157/1000], Train Loss (t_loss): 835.1127, Val Loss (v_loss): 811.9624\n",
      "Epoch [158/1000], Train Loss (t_loss): 812.0822, Val Loss (v_loss): 850.5180\n",
      "Epoch [159/1000], Train Loss (t_loss): 817.0043, Val Loss (v_loss): 936.2473\n",
      "Epoch [160/1000], Train Loss (t_loss): 768.5402, Val Loss (v_loss): 856.1004\n",
      "Epoch [161/1000], Train Loss (t_loss): 835.3433, Val Loss (v_loss): 643.0144\n",
      "Epoch [162/1000], Train Loss (t_loss): 849.2792, Val Loss (v_loss): 756.5974\n",
      "Epoch [163/1000], Train Loss (t_loss): 818.7815, Val Loss (v_loss): 731.2474\n",
      "Epoch [164/1000], Train Loss (t_loss): 799.0178, Val Loss (v_loss): 842.3655\n",
      "Epoch [165/1000], Train Loss (t_loss): 802.7722, Val Loss (v_loss): 752.9761\n",
      "Epoch [166/1000], Train Loss (t_loss): 803.3181, Val Loss (v_loss): 714.4032\n",
      "Epoch [167/1000], Train Loss (t_loss): 768.9958, Val Loss (v_loss): 780.8887\n",
      "Epoch [168/1000], Train Loss (t_loss): 820.5485, Val Loss (v_loss): 842.4994\n",
      "Epoch [169/1000], Train Loss (t_loss): 844.8653, Val Loss (v_loss): 812.7344\n",
      "Epoch [170/1000], Train Loss (t_loss): 769.9696, Val Loss (v_loss): 1065.5122\n",
      "Epoch [171/1000], Train Loss (t_loss): 790.5810, Val Loss (v_loss): 667.3802\n",
      "Epoch [172/1000], Train Loss (t_loss): 878.9878, Val Loss (v_loss): 667.4065\n",
      "Epoch [173/1000], Train Loss (t_loss): 799.5804, Val Loss (v_loss): 908.7154\n",
      "Epoch [174/1000], Train Loss (t_loss): 792.6648, Val Loss (v_loss): 735.0355\n",
      "Epoch [175/1000], Train Loss (t_loss): 723.6273, Val Loss (v_loss): 676.0507\n",
      "Epoch [176/1000], Train Loss (t_loss): 800.8534, Val Loss (v_loss): 744.7412\n",
      "Epoch [177/1000], Train Loss (t_loss): 769.7686, Val Loss (v_loss): 754.5203\n",
      "Epoch [178/1000], Train Loss (t_loss): 788.8640, Val Loss (v_loss): 822.0473\n",
      "Epoch [179/1000], Train Loss (t_loss): 768.1208, Val Loss (v_loss): 728.5791\n",
      "Epoch [180/1000], Train Loss (t_loss): 811.5650, Val Loss (v_loss): 761.5814\n",
      "Epoch [181/1000], Train Loss (t_loss): 758.5121, Val Loss (v_loss): 693.1744\n",
      "Epoch [182/1000], Train Loss (t_loss): 759.9975, Val Loss (v_loss): 792.2666\n",
      "Epoch [183/1000], Train Loss (t_loss): 827.0332, Val Loss (v_loss): 852.0240\n",
      "Epoch [184/1000], Train Loss (t_loss): 773.4594, Val Loss (v_loss): 702.2799\n",
      "Epoch [185/1000], Train Loss (t_loss): 722.5279, Val Loss (v_loss): 765.6047\n",
      "Epoch [186/1000], Train Loss (t_loss): 764.1467, Val Loss (v_loss): 783.4983\n",
      "Epoch [187/1000], Train Loss (t_loss): 787.0325, Val Loss (v_loss): 823.0698\n",
      "Epoch [188/1000], Train Loss (t_loss): 736.6752, Val Loss (v_loss): 852.2306\n",
      "Epoch [189/1000], Train Loss (t_loss): 815.9602, Val Loss (v_loss): 744.1330\n",
      "Epoch [190/1000], Train Loss (t_loss): 791.1142, Val Loss (v_loss): 721.0687\n",
      "Epoch [191/1000], Train Loss (t_loss): 732.3078, Val Loss (v_loss): 777.9909\n",
      "Epoch [192/1000], Train Loss (t_loss): 760.8667, Val Loss (v_loss): 810.7283\n",
      "Epoch [193/1000], Train Loss (t_loss): 797.3114, Val Loss (v_loss): 753.5021\n",
      "Epoch [194/1000], Train Loss (t_loss): 814.0003, Val Loss (v_loss): 757.8790\n",
      "Epoch [195/1000], Train Loss (t_loss): 747.7730, Val Loss (v_loss): 644.6361\n",
      "Epoch [196/1000], Train Loss (t_loss): 706.1697, Val Loss (v_loss): 681.3990\n",
      "Epoch [197/1000], Train Loss (t_loss): 784.7787, Val Loss (v_loss): 719.5745\n",
      "Epoch [198/1000], Train Loss (t_loss): 787.2625, Val Loss (v_loss): 987.0286\n",
      "Epoch [199/1000], Train Loss (t_loss): 771.2538, Val Loss (v_loss): 677.6113\n",
      "Epoch [200/1000], Train Loss (t_loss): 718.7794, Val Loss (v_loss): 709.2807\n",
      "Epoch [201/1000], Train Loss (t_loss): 807.4023, Val Loss (v_loss): 769.0511\n",
      "Epoch [202/1000], Train Loss (t_loss): 750.1724, Val Loss (v_loss): 716.5276\n",
      "Epoch [203/1000], Train Loss (t_loss): 799.5143, Val Loss (v_loss): 797.4472\n",
      "Epoch [204/1000], Train Loss (t_loss): 769.2819, Val Loss (v_loss): 647.8628\n",
      "Epoch [205/1000], Train Loss (t_loss): 775.4474, Val Loss (v_loss): 681.3525\n",
      "Epoch [206/1000], Train Loss (t_loss): 722.1657, Val Loss (v_loss): 684.2068\n",
      "Epoch [207/1000], Train Loss (t_loss): 720.9062, Val Loss (v_loss): 702.0505\n",
      "Epoch [208/1000], Train Loss (t_loss): 750.3385, Val Loss (v_loss): 742.5419\n",
      "Epoch [209/1000], Train Loss (t_loss): 698.0514, Val Loss (v_loss): 668.4453\n",
      "Epoch [210/1000], Train Loss (t_loss): 772.3007, Val Loss (v_loss): 610.8492\n",
      "Epoch [211/1000], Train Loss (t_loss): 746.4562, Val Loss (v_loss): 856.9791\n",
      "Epoch [212/1000], Train Loss (t_loss): 724.5420, Val Loss (v_loss): 699.7879\n",
      "Epoch [213/1000], Train Loss (t_loss): 703.9786, Val Loss (v_loss): 641.5723\n",
      "Epoch [214/1000], Train Loss (t_loss): 754.1881, Val Loss (v_loss): 710.3006\n",
      "Epoch [215/1000], Train Loss (t_loss): 741.1432, Val Loss (v_loss): 733.6800\n",
      "Epoch [216/1000], Train Loss (t_loss): 786.2217, Val Loss (v_loss): 800.2667\n",
      "Epoch [217/1000], Train Loss (t_loss): 731.4353, Val Loss (v_loss): 707.2674\n",
      "Epoch [218/1000], Train Loss (t_loss): 748.6613, Val Loss (v_loss): 687.8200\n",
      "Epoch [219/1000], Train Loss (t_loss): 741.6073, Val Loss (v_loss): 655.3779\n",
      "Epoch [220/1000], Train Loss (t_loss): 794.3747, Val Loss (v_loss): 601.0550\n",
      "Epoch [221/1000], Train Loss (t_loss): 760.9835, Val Loss (v_loss): 1132.0820\n",
      "Epoch [222/1000], Train Loss (t_loss): 779.6277, Val Loss (v_loss): 794.0504\n",
      "Epoch [223/1000], Train Loss (t_loss): 762.3384, Val Loss (v_loss): 739.9759\n",
      "Epoch [224/1000], Train Loss (t_loss): 712.8832, Val Loss (v_loss): 611.5177\n",
      "Epoch [225/1000], Train Loss (t_loss): 708.7448, Val Loss (v_loss): 687.0042\n",
      "Epoch [226/1000], Train Loss (t_loss): 719.8548, Val Loss (v_loss): 787.2406\n",
      "Epoch [227/1000], Train Loss (t_loss): 785.4255, Val Loss (v_loss): 905.0118\n",
      "Epoch [228/1000], Train Loss (t_loss): 761.0321, Val Loss (v_loss): 660.4152\n",
      "Epoch [229/1000], Train Loss (t_loss): 700.9403, Val Loss (v_loss): 729.3138\n",
      "Epoch [230/1000], Train Loss (t_loss): 691.4874, Val Loss (v_loss): 736.7847\n",
      "Epoch [231/1000], Train Loss (t_loss): 725.5891, Val Loss (v_loss): 797.1348\n",
      "Epoch [232/1000], Train Loss (t_loss): 736.0461, Val Loss (v_loss): 709.0570\n",
      "Epoch [233/1000], Train Loss (t_loss): 687.9031, Val Loss (v_loss): 847.5167\n",
      "Epoch [234/1000], Train Loss (t_loss): 734.6977, Val Loss (v_loss): 680.4534\n",
      "Epoch [235/1000], Train Loss (t_loss): 756.8593, Val Loss (v_loss): 818.5377\n",
      "Epoch [236/1000], Train Loss (t_loss): 726.3633, Val Loss (v_loss): 731.7331\n",
      "Epoch [237/1000], Train Loss (t_loss): 692.0714, Val Loss (v_loss): 827.4759\n",
      "Epoch [238/1000], Train Loss (t_loss): 723.8266, Val Loss (v_loss): 690.6367\n",
      "Epoch [239/1000], Train Loss (t_loss): 720.4562, Val Loss (v_loss): 564.6072\n",
      "Epoch [240/1000], Train Loss (t_loss): 696.2768, Val Loss (v_loss): 686.3841\n",
      "Epoch [241/1000], Train Loss (t_loss): 752.1714, Val Loss (v_loss): 615.9234\n",
      "Epoch [242/1000], Train Loss (t_loss): 763.9266, Val Loss (v_loss): 774.7336\n",
      "Epoch [243/1000], Train Loss (t_loss): 745.3132, Val Loss (v_loss): 836.1541\n",
      "Epoch [244/1000], Train Loss (t_loss): 758.3762, Val Loss (v_loss): 605.5614\n",
      "Epoch [245/1000], Train Loss (t_loss): 714.4746, Val Loss (v_loss): 691.8839\n",
      "Epoch [246/1000], Train Loss (t_loss): 694.6492, Val Loss (v_loss): 638.4812\n",
      "Epoch [247/1000], Train Loss (t_loss): 738.2660, Val Loss (v_loss): 647.5803\n",
      "Epoch [248/1000], Train Loss (t_loss): 718.7209, Val Loss (v_loss): 682.3477\n",
      "Epoch [249/1000], Train Loss (t_loss): 735.3214, Val Loss (v_loss): 663.3501\n",
      "Epoch [250/1000], Train Loss (t_loss): 680.7437, Val Loss (v_loss): 572.7469\n",
      "Epoch [251/1000], Train Loss (t_loss): 680.7237, Val Loss (v_loss): 738.6737\n",
      "Epoch [252/1000], Train Loss (t_loss): 673.8674, Val Loss (v_loss): 721.4784\n",
      "Epoch [253/1000], Train Loss (t_loss): 716.7666, Val Loss (v_loss): 588.5685\n",
      "Epoch [254/1000], Train Loss (t_loss): 721.2527, Val Loss (v_loss): 909.9532\n",
      "Epoch [255/1000], Train Loss (t_loss): 702.6255, Val Loss (v_loss): 643.6358\n",
      "Epoch [256/1000], Train Loss (t_loss): 670.5054, Val Loss (v_loss): 711.2638\n",
      "Epoch [257/1000], Train Loss (t_loss): 661.9275, Val Loss (v_loss): 631.1505\n",
      "Epoch [258/1000], Train Loss (t_loss): 731.2423, Val Loss (v_loss): 614.4667\n",
      "Epoch [259/1000], Train Loss (t_loss): 769.3737, Val Loss (v_loss): 791.3959\n",
      "Epoch [260/1000], Train Loss (t_loss): 721.3954, Val Loss (v_loss): 734.7770\n",
      "Epoch [261/1000], Train Loss (t_loss): 711.6818, Val Loss (v_loss): 640.1088\n",
      "Epoch [262/1000], Train Loss (t_loss): 687.3890, Val Loss (v_loss): 688.3489\n",
      "Epoch [263/1000], Train Loss (t_loss): 674.4583, Val Loss (v_loss): 605.1402\n",
      "Epoch [264/1000], Train Loss (t_loss): 682.1935, Val Loss (v_loss): 690.4758\n",
      "Epoch [265/1000], Train Loss (t_loss): 706.5222, Val Loss (v_loss): 767.1660\n",
      "Epoch [266/1000], Train Loss (t_loss): 682.8347, Val Loss (v_loss): 629.4767\n",
      "Epoch [267/1000], Train Loss (t_loss): 727.8997, Val Loss (v_loss): 702.1054\n",
      "Epoch [268/1000], Train Loss (t_loss): 715.9101, Val Loss (v_loss): 774.6304\n",
      "Epoch [269/1000], Train Loss (t_loss): 695.0596, Val Loss (v_loss): 797.5437\n",
      "Epoch [270/1000], Train Loss (t_loss): 672.9363, Val Loss (v_loss): 594.3561\n",
      "Epoch [271/1000], Train Loss (t_loss): 727.9343, Val Loss (v_loss): 657.3324\n",
      "Epoch [272/1000], Train Loss (t_loss): 689.7631, Val Loss (v_loss): 839.9577\n",
      "Epoch [273/1000], Train Loss (t_loss): 681.1751, Val Loss (v_loss): 679.5729\n",
      "Epoch [274/1000], Train Loss (t_loss): 670.7746, Val Loss (v_loss): 653.3105\n",
      "Epoch [275/1000], Train Loss (t_loss): 623.7472, Val Loss (v_loss): 612.6095\n",
      "Epoch [276/1000], Train Loss (t_loss): 692.6823, Val Loss (v_loss): 619.5019\n",
      "Epoch [277/1000], Train Loss (t_loss): 682.3024, Val Loss (v_loss): 875.5825\n",
      "Epoch [278/1000], Train Loss (t_loss): 717.3391, Val Loss (v_loss): 833.5866\n",
      "Epoch [279/1000], Train Loss (t_loss): 689.4837, Val Loss (v_loss): 637.1103\n",
      "Epoch [280/1000], Train Loss (t_loss): 628.0541, Val Loss (v_loss): 626.2572\n",
      "Epoch [281/1000], Train Loss (t_loss): 664.6066, Val Loss (v_loss): 605.3832\n",
      "Epoch [282/1000], Train Loss (t_loss): 714.7914, Val Loss (v_loss): 781.3318\n",
      "Epoch [283/1000], Train Loss (t_loss): 697.9849, Val Loss (v_loss): 649.4612\n",
      "Epoch [284/1000], Train Loss (t_loss): 693.2788, Val Loss (v_loss): 584.3078\n",
      "Epoch [285/1000], Train Loss (t_loss): 648.5269, Val Loss (v_loss): 718.3769\n",
      "Epoch [286/1000], Train Loss (t_loss): 665.4282, Val Loss (v_loss): 722.0217\n",
      "Epoch [287/1000], Train Loss (t_loss): 649.4146, Val Loss (v_loss): 640.9002\n",
      "Epoch [288/1000], Train Loss (t_loss): 605.9452, Val Loss (v_loss): 625.9034\n",
      "Epoch [289/1000], Train Loss (t_loss): 634.0510, Val Loss (v_loss): 795.5609\n",
      "Epoch [290/1000], Train Loss (t_loss): 685.4705, Val Loss (v_loss): 711.0793\n",
      "Epoch [291/1000], Train Loss (t_loss): 650.1508, Val Loss (v_loss): 658.2172\n",
      "Epoch [292/1000], Train Loss (t_loss): 653.5206, Val Loss (v_loss): 580.6058\n",
      "Epoch [293/1000], Train Loss (t_loss): 637.9808, Val Loss (v_loss): 736.4159\n",
      "Epoch [294/1000], Train Loss (t_loss): 646.9192, Val Loss (v_loss): 678.0526\n",
      "Epoch [295/1000], Train Loss (t_loss): 713.5628, Val Loss (v_loss): 576.7216\n",
      "Epoch [296/1000], Train Loss (t_loss): 634.1383, Val Loss (v_loss): 511.6736\n",
      "Epoch [297/1000], Train Loss (t_loss): 670.5119, Val Loss (v_loss): 584.2134\n",
      "Epoch [298/1000], Train Loss (t_loss): 640.1222, Val Loss (v_loss): 693.2549\n",
      "Epoch [299/1000], Train Loss (t_loss): 671.6226, Val Loss (v_loss): 723.4560\n",
      "Epoch [300/1000], Train Loss (t_loss): 713.0261, Val Loss (v_loss): 638.2037\n",
      "Epoch [301/1000], Train Loss (t_loss): 673.9909, Val Loss (v_loss): 656.9036\n",
      "Epoch [302/1000], Train Loss (t_loss): 643.4036, Val Loss (v_loss): 677.8623\n",
      "Epoch [303/1000], Train Loss (t_loss): 662.9142, Val Loss (v_loss): 597.4969\n",
      "Epoch [304/1000], Train Loss (t_loss): 599.7911, Val Loss (v_loss): 667.5514\n",
      "Epoch [305/1000], Train Loss (t_loss): 664.5208, Val Loss (v_loss): 806.2351\n",
      "Epoch [306/1000], Train Loss (t_loss): 651.6265, Val Loss (v_loss): 632.6011\n",
      "Epoch [307/1000], Train Loss (t_loss): 661.9696, Val Loss (v_loss): 608.2159\n",
      "Epoch [308/1000], Train Loss (t_loss): 594.2723, Val Loss (v_loss): 673.5313\n",
      "Epoch [309/1000], Train Loss (t_loss): 638.0032, Val Loss (v_loss): 657.7988\n",
      "Epoch [310/1000], Train Loss (t_loss): 671.1242, Val Loss (v_loss): 695.5888\n",
      "Epoch [311/1000], Train Loss (t_loss): 630.9914, Val Loss (v_loss): 564.9661\n",
      "Epoch [312/1000], Train Loss (t_loss): 668.6754, Val Loss (v_loss): 605.6098\n",
      "Epoch [313/1000], Train Loss (t_loss): 655.7336, Val Loss (v_loss): 640.9916\n",
      "Epoch [314/1000], Train Loss (t_loss): 610.6422, Val Loss (v_loss): 522.9691\n",
      "Epoch [315/1000], Train Loss (t_loss): 673.2517, Val Loss (v_loss): 513.3428\n",
      "Epoch [316/1000], Train Loss (t_loss): 673.3855, Val Loss (v_loss): 547.0964\n",
      "Epoch [317/1000], Train Loss (t_loss): 584.4707, Val Loss (v_loss): 758.9421\n",
      "Epoch [318/1000], Train Loss (t_loss): 643.5006, Val Loss (v_loss): 803.7274\n",
      "Epoch [319/1000], Train Loss (t_loss): 723.2048, Val Loss (v_loss): 754.3302\n",
      "Epoch [320/1000], Train Loss (t_loss): 706.3763, Val Loss (v_loss): 591.2847\n",
      "Epoch [321/1000], Train Loss (t_loss): 596.5461, Val Loss (v_loss): 507.7977\n",
      "Epoch [322/1000], Train Loss (t_loss): 698.4714, Val Loss (v_loss): 593.7269\n",
      "Epoch [323/1000], Train Loss (t_loss): 683.3832, Val Loss (v_loss): 654.0991\n",
      "Epoch [324/1000], Train Loss (t_loss): 603.4146, Val Loss (v_loss): 570.1911\n",
      "Epoch [325/1000], Train Loss (t_loss): 639.3206, Val Loss (v_loss): 665.8158\n",
      "Epoch [326/1000], Train Loss (t_loss): 640.8740, Val Loss (v_loss): 862.8945\n",
      "Epoch [327/1000], Train Loss (t_loss): 670.6180, Val Loss (v_loss): 800.6729\n",
      "Epoch [328/1000], Train Loss (t_loss): 617.1852, Val Loss (v_loss): 633.6821\n",
      "Epoch [329/1000], Train Loss (t_loss): 615.8383, Val Loss (v_loss): 527.4760\n",
      "Epoch [330/1000], Train Loss (t_loss): 610.6672, Val Loss (v_loss): 718.9332\n",
      "Epoch [331/1000], Train Loss (t_loss): 648.6516, Val Loss (v_loss): 617.6319\n",
      "Epoch [332/1000], Train Loss (t_loss): 625.4533, Val Loss (v_loss): 588.8582\n",
      "Epoch [333/1000], Train Loss (t_loss): 616.5408, Val Loss (v_loss): 598.9285\n",
      "Epoch [334/1000], Train Loss (t_loss): 600.1285, Val Loss (v_loss): 529.8856\n",
      "Epoch [335/1000], Train Loss (t_loss): 625.9923, Val Loss (v_loss): 547.5193\n",
      "Epoch [336/1000], Train Loss (t_loss): 596.8643, Val Loss (v_loss): 599.5977\n",
      "Epoch [337/1000], Train Loss (t_loss): 607.8610, Val Loss (v_loss): 579.0833\n",
      "Epoch [338/1000], Train Loss (t_loss): 588.5045, Val Loss (v_loss): 613.5829\n",
      "Epoch [339/1000], Train Loss (t_loss): 614.0929, Val Loss (v_loss): 493.8756\n",
      "Epoch [340/1000], Train Loss (t_loss): 658.6372, Val Loss (v_loss): 554.0034\n",
      "Epoch [341/1000], Train Loss (t_loss): 620.6661, Val Loss (v_loss): 459.6279\n",
      "Epoch [342/1000], Train Loss (t_loss): 641.4918, Val Loss (v_loss): 496.3182\n",
      "Epoch [343/1000], Train Loss (t_loss): 534.3141, Val Loss (v_loss): 501.9001\n",
      "Epoch [344/1000], Train Loss (t_loss): 704.7092, Val Loss (v_loss): 495.9261\n",
      "Epoch [345/1000], Train Loss (t_loss): 622.7630, Val Loss (v_loss): 556.0717\n",
      "Epoch [346/1000], Train Loss (t_loss): 574.4098, Val Loss (v_loss): 529.1314\n",
      "Epoch [347/1000], Train Loss (t_loss): 575.3476, Val Loss (v_loss): 604.1634\n",
      "Epoch [348/1000], Train Loss (t_loss): 548.6405, Val Loss (v_loss): 556.6651\n",
      "Epoch [349/1000], Train Loss (t_loss): 601.4487, Val Loss (v_loss): 535.5232\n",
      "Epoch [350/1000], Train Loss (t_loss): 608.7905, Val Loss (v_loss): 623.0579\n",
      "Epoch [351/1000], Train Loss (t_loss): 580.7309, Val Loss (v_loss): 597.6402\n",
      "Epoch [352/1000], Train Loss (t_loss): 592.1105, Val Loss (v_loss): 504.3829\n",
      "Epoch [353/1000], Train Loss (t_loss): 593.6839, Val Loss (v_loss): 493.7962\n",
      "Epoch [354/1000], Train Loss (t_loss): 610.8368, Val Loss (v_loss): 598.3490\n",
      "Epoch [355/1000], Train Loss (t_loss): 578.3108, Val Loss (v_loss): 644.0013\n",
      "Epoch [356/1000], Train Loss (t_loss): 582.2141, Val Loss (v_loss): 546.8498\n",
      "Epoch [357/1000], Train Loss (t_loss): 550.1184, Val Loss (v_loss): 621.8492\n",
      "Epoch [358/1000], Train Loss (t_loss): 623.0720, Val Loss (v_loss): 589.3070\n",
      "Epoch [359/1000], Train Loss (t_loss): 581.1300, Val Loss (v_loss): 493.8455\n",
      "Epoch [360/1000], Train Loss (t_loss): 632.8032, Val Loss (v_loss): 469.1952\n",
      "Epoch [361/1000], Train Loss (t_loss): 605.4544, Val Loss (v_loss): 442.8538\n",
      "Epoch [362/1000], Train Loss (t_loss): 576.1082, Val Loss (v_loss): 575.1192\n",
      "Epoch [363/1000], Train Loss (t_loss): 533.6962, Val Loss (v_loss): 488.5170\n",
      "Epoch [364/1000], Train Loss (t_loss): 519.5946, Val Loss (v_loss): 511.8162\n",
      "Epoch [365/1000], Train Loss (t_loss): 592.1977, Val Loss (v_loss): 905.3158\n",
      "Epoch [366/1000], Train Loss (t_loss): 583.9140, Val Loss (v_loss): 551.3715\n",
      "Epoch [367/1000], Train Loss (t_loss): 543.4845, Val Loss (v_loss): 732.1010\n",
      "Epoch [368/1000], Train Loss (t_loss): 548.8614, Val Loss (v_loss): 463.9840\n",
      "Epoch [369/1000], Train Loss (t_loss): 556.9596, Val Loss (v_loss): 611.9715\n",
      "Epoch [370/1000], Train Loss (t_loss): 496.7283, Val Loss (v_loss): 496.0632\n",
      "Epoch [371/1000], Train Loss (t_loss): 508.8781, Val Loss (v_loss): 549.8849\n",
      "Epoch [372/1000], Train Loss (t_loss): 504.6490, Val Loss (v_loss): 472.5819\n",
      "Epoch [373/1000], Train Loss (t_loss): 541.6710, Val Loss (v_loss): 480.4780\n",
      "Epoch [374/1000], Train Loss (t_loss): 516.0025, Val Loss (v_loss): 476.4532\n",
      "Epoch [375/1000], Train Loss (t_loss): 528.5554, Val Loss (v_loss): 418.9584\n",
      "Epoch [376/1000], Train Loss (t_loss): 671.4913, Val Loss (v_loss): 460.2668\n",
      "Epoch [377/1000], Train Loss (t_loss): 491.7199, Val Loss (v_loss): 511.8454\n",
      "Epoch [378/1000], Train Loss (t_loss): 577.1152, Val Loss (v_loss): 578.7207\n",
      "Epoch [379/1000], Train Loss (t_loss): 523.9798, Val Loss (v_loss): 425.0948\n",
      "Epoch [380/1000], Train Loss (t_loss): 521.2394, Val Loss (v_loss): 452.9156\n",
      "Epoch [381/1000], Train Loss (t_loss): 522.2655, Val Loss (v_loss): 425.2181\n",
      "Epoch [382/1000], Train Loss (t_loss): 578.6597, Val Loss (v_loss): 414.9773\n",
      "Epoch [383/1000], Train Loss (t_loss): 511.1339, Val Loss (v_loss): 396.1267\n",
      "Epoch [384/1000], Train Loss (t_loss): 564.4978, Val Loss (v_loss): 451.5932\n",
      "Epoch [385/1000], Train Loss (t_loss): 477.1330, Val Loss (v_loss): 437.5487\n",
      "Epoch [386/1000], Train Loss (t_loss): 483.2341, Val Loss (v_loss): 445.3188\n",
      "Epoch [387/1000], Train Loss (t_loss): 499.6002, Val Loss (v_loss): 377.3148\n",
      "Epoch [388/1000], Train Loss (t_loss): 621.6351, Val Loss (v_loss): 398.9234\n",
      "Epoch [389/1000], Train Loss (t_loss): 697.7243, Val Loss (v_loss): 583.7156\n",
      "Epoch [390/1000], Train Loss (t_loss): 638.8098, Val Loss (v_loss): 668.8938\n",
      "Epoch [391/1000], Train Loss (t_loss): 544.2736, Val Loss (v_loss): 548.2852\n",
      "Epoch [392/1000], Train Loss (t_loss): 516.1416, Val Loss (v_loss): 531.9655\n",
      "Epoch [393/1000], Train Loss (t_loss): 431.1995, Val Loss (v_loss): 545.6628\n",
      "Epoch [394/1000], Train Loss (t_loss): 531.4586, Val Loss (v_loss): 455.0947\n",
      "Epoch [395/1000], Train Loss (t_loss): 485.9554, Val Loss (v_loss): 443.5819\n",
      "Epoch [396/1000], Train Loss (t_loss): 502.7255, Val Loss (v_loss): 412.7036\n",
      "Epoch [397/1000], Train Loss (t_loss): 480.3197, Val Loss (v_loss): 608.6816\n",
      "Epoch [398/1000], Train Loss (t_loss): 507.1450, Val Loss (v_loss): 391.8799\n",
      "Epoch [399/1000], Train Loss (t_loss): 454.0189, Val Loss (v_loss): 393.3056\n",
      "Epoch [400/1000], Train Loss (t_loss): 487.1459, Val Loss (v_loss): 502.1624\n",
      "Epoch [401/1000], Train Loss (t_loss): 522.6230, Val Loss (v_loss): 456.7643\n",
      "Epoch [402/1000], Train Loss (t_loss): 456.7995, Val Loss (v_loss): 448.6004\n",
      "Epoch [403/1000], Train Loss (t_loss): 448.0486, Val Loss (v_loss): 357.7106\n",
      "Epoch [404/1000], Train Loss (t_loss): 480.5413, Val Loss (v_loss): 379.8366\n",
      "Epoch [405/1000], Train Loss (t_loss): 445.7872, Val Loss (v_loss): 388.4323\n",
      "Epoch [406/1000], Train Loss (t_loss): 421.1744, Val Loss (v_loss): 349.4137\n",
      "Epoch [407/1000], Train Loss (t_loss): 488.9133, Val Loss (v_loss): 381.2559\n",
      "Epoch [408/1000], Train Loss (t_loss): 463.2907, Val Loss (v_loss): 386.1238\n",
      "Epoch [409/1000], Train Loss (t_loss): 435.6699, Val Loss (v_loss): 485.5246\n",
      "Epoch [410/1000], Train Loss (t_loss): 452.9799, Val Loss (v_loss): 449.1247\n",
      "Epoch [411/1000], Train Loss (t_loss): 447.6315, Val Loss (v_loss): 355.2991\n",
      "Epoch [412/1000], Train Loss (t_loss): 413.6273, Val Loss (v_loss): 340.3724\n",
      "Epoch [413/1000], Train Loss (t_loss): 442.0666, Val Loss (v_loss): 395.2842\n",
      "Epoch [414/1000], Train Loss (t_loss): 454.0974, Val Loss (v_loss): 440.9281\n",
      "Epoch [415/1000], Train Loss (t_loss): 450.5440, Val Loss (v_loss): 414.4948\n",
      "Epoch [416/1000], Train Loss (t_loss): 449.7032, Val Loss (v_loss): 365.9991\n",
      "Epoch [417/1000], Train Loss (t_loss): 442.1566, Val Loss (v_loss): 376.2879\n",
      "Epoch [418/1000], Train Loss (t_loss): 459.4449, Val Loss (v_loss): 338.7563\n",
      "Epoch [419/1000], Train Loss (t_loss): 449.6794, Val Loss (v_loss): 345.0650\n",
      "Epoch [420/1000], Train Loss (t_loss): 482.3217, Val Loss (v_loss): 353.8330\n",
      "Epoch [421/1000], Train Loss (t_loss): 475.9153, Val Loss (v_loss): 361.8798\n",
      "Epoch [422/1000], Train Loss (t_loss): 428.0499, Val Loss (v_loss): 337.2887\n",
      "Epoch [423/1000], Train Loss (t_loss): 468.8274, Val Loss (v_loss): 435.2991\n",
      "Epoch [424/1000], Train Loss (t_loss): 437.9892, Val Loss (v_loss): 448.5815\n",
      "Epoch [425/1000], Train Loss (t_loss): 455.1021, Val Loss (v_loss): 402.9757\n",
      "Epoch [426/1000], Train Loss (t_loss): 433.1807, Val Loss (v_loss): 516.5077\n",
      "Epoch [427/1000], Train Loss (t_loss): 449.8090, Val Loss (v_loss): 601.2755\n",
      "Epoch [428/1000], Train Loss (t_loss): 446.4088, Val Loss (v_loss): 629.6927\n",
      "Epoch [429/1000], Train Loss (t_loss): 441.9135, Val Loss (v_loss): 351.8139\n",
      "Epoch [430/1000], Train Loss (t_loss): 438.4549, Val Loss (v_loss): 375.4663\n",
      "Epoch [431/1000], Train Loss (t_loss): 413.5012, Val Loss (v_loss): 347.2769\n",
      "Epoch [432/1000], Train Loss (t_loss): 472.8545, Val Loss (v_loss): 503.9724\n",
      "Epoch [433/1000], Train Loss (t_loss): 437.2375, Val Loss (v_loss): 416.8426\n",
      "Epoch [434/1000], Train Loss (t_loss): 415.4547, Val Loss (v_loss): 357.8458\n",
      "Epoch [435/1000], Train Loss (t_loss): 403.9666, Val Loss (v_loss): 344.1425\n",
      "Epoch [436/1000], Train Loss (t_loss): 439.1109, Val Loss (v_loss): 365.9942\n",
      "Epoch [437/1000], Train Loss (t_loss): 379.5114, Val Loss (v_loss): 421.3691\n",
      "Epoch [438/1000], Train Loss (t_loss): 392.8956, Val Loss (v_loss): 389.6972\n",
      "Epoch [439/1000], Train Loss (t_loss): 425.9316, Val Loss (v_loss): 315.1347\n",
      "Epoch [440/1000], Train Loss (t_loss): 422.0837, Val Loss (v_loss): 311.2400\n",
      "Epoch [441/1000], Train Loss (t_loss): 419.5379, Val Loss (v_loss): 288.8978\n",
      "Epoch [442/1000], Train Loss (t_loss): 376.0993, Val Loss (v_loss): 273.6027\n",
      "Epoch [443/1000], Train Loss (t_loss): 362.7598, Val Loss (v_loss): 273.5635\n",
      "Epoch [444/1000], Train Loss (t_loss): 357.0590, Val Loss (v_loss): 400.2890\n",
      "Epoch [445/1000], Train Loss (t_loss): 352.1357, Val Loss (v_loss): 335.1087\n",
      "Epoch [446/1000], Train Loss (t_loss): 381.5249, Val Loss (v_loss): 437.8749\n",
      "Epoch [447/1000], Train Loss (t_loss): 409.1826, Val Loss (v_loss): 412.6193\n",
      "Epoch [448/1000], Train Loss (t_loss): 419.4402, Val Loss (v_loss): 269.2867\n",
      "Epoch [449/1000], Train Loss (t_loss): 375.5981, Val Loss (v_loss): 357.7340\n",
      "Epoch [450/1000], Train Loss (t_loss): 348.6644, Val Loss (v_loss): 423.6842\n",
      "Epoch [451/1000], Train Loss (t_loss): 347.5360, Val Loss (v_loss): 328.1845\n",
      "Epoch [452/1000], Train Loss (t_loss): 339.0529, Val Loss (v_loss): 260.3576\n",
      "Epoch [453/1000], Train Loss (t_loss): 334.5576, Val Loss (v_loss): 247.6404\n",
      "Epoch [454/1000], Train Loss (t_loss): 320.0982, Val Loss (v_loss): 301.9577\n",
      "Epoch [455/1000], Train Loss (t_loss): 346.6751, Val Loss (v_loss): 265.1120\n",
      "Epoch [456/1000], Train Loss (t_loss): 329.6527, Val Loss (v_loss): 289.0884\n",
      "Epoch [457/1000], Train Loss (t_loss): 328.0604, Val Loss (v_loss): 409.1905\n",
      "Epoch [458/1000], Train Loss (t_loss): 348.0892, Val Loss (v_loss): 314.2666\n",
      "Epoch [459/1000], Train Loss (t_loss): 339.7507, Val Loss (v_loss): 267.4114\n",
      "Epoch [460/1000], Train Loss (t_loss): 335.0522, Val Loss (v_loss): 368.0929\n",
      "Epoch [461/1000], Train Loss (t_loss): 325.3288, Val Loss (v_loss): 274.2134\n",
      "Epoch [462/1000], Train Loss (t_loss): 332.6925, Val Loss (v_loss): 257.0782\n",
      "Epoch [463/1000], Train Loss (t_loss): 301.2612, Val Loss (v_loss): 272.0247\n",
      "Epoch [464/1000], Train Loss (t_loss): 341.9495, Val Loss (v_loss): 383.4519\n",
      "Epoch [465/1000], Train Loss (t_loss): 302.7200, Val Loss (v_loss): 376.9249\n",
      "Epoch [466/1000], Train Loss (t_loss): 363.4929, Val Loss (v_loss): 338.7095\n",
      "Epoch [467/1000], Train Loss (t_loss): 323.9831, Val Loss (v_loss): 396.4678\n",
      "Epoch [468/1000], Train Loss (t_loss): 327.2866, Val Loss (v_loss): 219.6349\n",
      "Epoch [469/1000], Train Loss (t_loss): 337.6530, Val Loss (v_loss): 269.6884\n",
      "Epoch [470/1000], Train Loss (t_loss): 300.3276, Val Loss (v_loss): 246.2967\n",
      "Epoch [471/1000], Train Loss (t_loss): 320.8718, Val Loss (v_loss): 235.1239\n",
      "Epoch [472/1000], Train Loss (t_loss): 324.7526, Val Loss (v_loss): 218.6594\n",
      "Epoch [473/1000], Train Loss (t_loss): 300.7641, Val Loss (v_loss): 253.6969\n",
      "Epoch [474/1000], Train Loss (t_loss): 327.4420, Val Loss (v_loss): 400.0631\n",
      "Epoch [475/1000], Train Loss (t_loss): 320.2138, Val Loss (v_loss): 219.8168\n",
      "Epoch [476/1000], Train Loss (t_loss): 352.7803, Val Loss (v_loss): 262.6123\n",
      "Epoch [477/1000], Train Loss (t_loss): 328.4138, Val Loss (v_loss): 336.7639\n",
      "Epoch [478/1000], Train Loss (t_loss): 286.7197, Val Loss (v_loss): 316.1203\n",
      "Epoch [479/1000], Train Loss (t_loss): 293.7865, Val Loss (v_loss): 292.2751\n",
      "Epoch [480/1000], Train Loss (t_loss): 278.9373, Val Loss (v_loss): 270.7803\n",
      "Epoch [481/1000], Train Loss (t_loss): 316.1462, Val Loss (v_loss): 269.8435\n",
      "Epoch [482/1000], Train Loss (t_loss): 293.5578, Val Loss (v_loss): 259.7595\n",
      "Epoch [483/1000], Train Loss (t_loss): 301.0406, Val Loss (v_loss): 239.0876\n",
      "Epoch [484/1000], Train Loss (t_loss): 292.0651, Val Loss (v_loss): 239.0419\n",
      "Epoch [485/1000], Train Loss (t_loss): 285.3094, Val Loss (v_loss): 221.3901\n",
      "Epoch [486/1000], Train Loss (t_loss): 300.4728, Val Loss (v_loss): 252.6881\n",
      "Epoch [487/1000], Train Loss (t_loss): 311.2633, Val Loss (v_loss): 253.2567\n",
      "Epoch [488/1000], Train Loss (t_loss): 273.5667, Val Loss (v_loss): 222.7799\n",
      "Epoch [489/1000], Train Loss (t_loss): 272.2226, Val Loss (v_loss): 223.3510\n",
      "Epoch [490/1000], Train Loss (t_loss): 277.1129, Val Loss (v_loss): 222.8165\n",
      "Epoch [491/1000], Train Loss (t_loss): 272.5989, Val Loss (v_loss): 185.1474\n",
      "Epoch [492/1000], Train Loss (t_loss): 253.2861, Val Loss (v_loss): 253.4459\n",
      "Epoch [493/1000], Train Loss (t_loss): 294.6494, Val Loss (v_loss): 207.3551\n",
      "Epoch [494/1000], Train Loss (t_loss): 282.4311, Val Loss (v_loss): 219.5235\n",
      "Epoch [495/1000], Train Loss (t_loss): 270.4298, Val Loss (v_loss): 202.0107\n",
      "Epoch [496/1000], Train Loss (t_loss): 279.4867, Val Loss (v_loss): 232.0268\n",
      "Epoch [497/1000], Train Loss (t_loss): 264.7719, Val Loss (v_loss): 255.8556\n",
      "Epoch [498/1000], Train Loss (t_loss): 251.2150, Val Loss (v_loss): 168.1829\n",
      "Epoch [499/1000], Train Loss (t_loss): 286.6150, Val Loss (v_loss): 174.6974\n",
      "Epoch [500/1000], Train Loss (t_loss): 296.0786, Val Loss (v_loss): 216.5931\n",
      "Epoch [501/1000], Train Loss (t_loss): 276.6033, Val Loss (v_loss): 345.3194\n",
      "Epoch [502/1000], Train Loss (t_loss): 302.7030, Val Loss (v_loss): 264.4582\n",
      "Epoch [503/1000], Train Loss (t_loss): 262.5501, Val Loss (v_loss): 404.9950\n",
      "Epoch [504/1000], Train Loss (t_loss): 271.8072, Val Loss (v_loss): 248.2601\n",
      "Epoch [505/1000], Train Loss (t_loss): 284.5273, Val Loss (v_loss): 328.4546\n",
      "Epoch [506/1000], Train Loss (t_loss): 278.0198, Val Loss (v_loss): 204.4048\n",
      "Epoch [507/1000], Train Loss (t_loss): 248.7894, Val Loss (v_loss): 208.0942\n",
      "Epoch [508/1000], Train Loss (t_loss): 243.0268, Val Loss (v_loss): 172.9810\n",
      "Epoch [509/1000], Train Loss (t_loss): 310.2293, Val Loss (v_loss): 369.0076\n",
      "Epoch [510/1000], Train Loss (t_loss): 286.8234, Val Loss (v_loss): 225.8732\n",
      "Epoch [511/1000], Train Loss (t_loss): 247.7316, Val Loss (v_loss): 191.6898\n",
      "Epoch [512/1000], Train Loss (t_loss): 247.1690, Val Loss (v_loss): 246.1967\n",
      "Epoch [513/1000], Train Loss (t_loss): 241.5764, Val Loss (v_loss): 251.1713\n",
      "Epoch [514/1000], Train Loss (t_loss): 248.2997, Val Loss (v_loss): 211.6799\n",
      "Epoch [515/1000], Train Loss (t_loss): 247.2704, Val Loss (v_loss): 166.4179\n",
      "Epoch [516/1000], Train Loss (t_loss): 269.3559, Val Loss (v_loss): 168.1106\n",
      "Epoch [517/1000], Train Loss (t_loss): 247.5808, Val Loss (v_loss): 173.6443\n",
      "Epoch [518/1000], Train Loss (t_loss): 233.1434, Val Loss (v_loss): 161.0989\n",
      "Epoch [519/1000], Train Loss (t_loss): 281.9222, Val Loss (v_loss): 209.5280\n",
      "Epoch [520/1000], Train Loss (t_loss): 269.2084, Val Loss (v_loss): 216.5193\n",
      "Epoch [521/1000], Train Loss (t_loss): 252.2835, Val Loss (v_loss): 209.3660\n",
      "Epoch [522/1000], Train Loss (t_loss): 241.1291, Val Loss (v_loss): 206.7661\n",
      "Epoch [523/1000], Train Loss (t_loss): 236.2117, Val Loss (v_loss): 278.0986\n",
      "Epoch [524/1000], Train Loss (t_loss): 253.6039, Val Loss (v_loss): 210.8971\n",
      "Epoch [525/1000], Train Loss (t_loss): 257.8969, Val Loss (v_loss): 228.1088\n",
      "Epoch [526/1000], Train Loss (t_loss): 234.1761, Val Loss (v_loss): 175.0268\n",
      "Epoch [527/1000], Train Loss (t_loss): 289.1337, Val Loss (v_loss): 254.0659\n",
      "Epoch [528/1000], Train Loss (t_loss): 263.5084, Val Loss (v_loss): 173.5433\n",
      "Epoch [529/1000], Train Loss (t_loss): 269.4182, Val Loss (v_loss): 182.7613\n",
      "Epoch [530/1000], Train Loss (t_loss): 233.1232, Val Loss (v_loss): 157.2038\n",
      "Epoch [531/1000], Train Loss (t_loss): 251.4359, Val Loss (v_loss): 161.3835\n",
      "Epoch [532/1000], Train Loss (t_loss): 229.6346, Val Loss (v_loss): 162.3172\n",
      "Epoch [533/1000], Train Loss (t_loss): 259.2712, Val Loss (v_loss): 200.8296\n",
      "Epoch [534/1000], Train Loss (t_loss): 238.6069, Val Loss (v_loss): 161.0327\n",
      "Epoch [535/1000], Train Loss (t_loss): 241.6625, Val Loss (v_loss): 197.2762\n",
      "Epoch [536/1000], Train Loss (t_loss): 250.0532, Val Loss (v_loss): 193.7192\n",
      "Epoch [537/1000], Train Loss (t_loss): 244.6815, Val Loss (v_loss): 167.0761\n",
      "Epoch [538/1000], Train Loss (t_loss): 235.8181, Val Loss (v_loss): 153.4662\n",
      "Epoch [539/1000], Train Loss (t_loss): 241.2230, Val Loss (v_loss): 197.8929\n",
      "Epoch [540/1000], Train Loss (t_loss): 247.3344, Val Loss (v_loss): 160.1499\n",
      "Epoch [541/1000], Train Loss (t_loss): 221.2008, Val Loss (v_loss): 143.2253\n",
      "Epoch [542/1000], Train Loss (t_loss): 228.2707, Val Loss (v_loss): 187.5710\n",
      "Epoch [543/1000], Train Loss (t_loss): 223.3932, Val Loss (v_loss): 185.1665\n",
      "Epoch [544/1000], Train Loss (t_loss): 242.6988, Val Loss (v_loss): 163.8398\n",
      "Epoch [545/1000], Train Loss (t_loss): 230.5009, Val Loss (v_loss): 140.3648\n",
      "Epoch [546/1000], Train Loss (t_loss): 249.4792, Val Loss (v_loss): 150.0891\n",
      "Epoch [547/1000], Train Loss (t_loss): 251.5609, Val Loss (v_loss): 210.6318\n",
      "Epoch [548/1000], Train Loss (t_loss): 236.4675, Val Loss (v_loss): 229.5528\n",
      "Epoch [549/1000], Train Loss (t_loss): 238.4825, Val Loss (v_loss): 204.6022\n",
      "Epoch [550/1000], Train Loss (t_loss): 250.8605, Val Loss (v_loss): 190.1790\n",
      "Epoch [551/1000], Train Loss (t_loss): 235.2480, Val Loss (v_loss): 211.8576\n",
      "Epoch [552/1000], Train Loss (t_loss): 256.3462, Val Loss (v_loss): 181.0240\n",
      "Epoch [553/1000], Train Loss (t_loss): 270.5312, Val Loss (v_loss): 220.4313\n",
      "Epoch [554/1000], Train Loss (t_loss): 265.9275, Val Loss (v_loss): 246.9916\n",
      "Epoch [555/1000], Train Loss (t_loss): 236.0027, Val Loss (v_loss): 182.5171\n",
      "Epoch [556/1000], Train Loss (t_loss): 234.0981, Val Loss (v_loss): 160.4328\n",
      "Epoch [557/1000], Train Loss (t_loss): 231.1415, Val Loss (v_loss): 142.5204\n",
      "Epoch [558/1000], Train Loss (t_loss): 228.7315, Val Loss (v_loss): 198.6987\n",
      "Epoch [559/1000], Train Loss (t_loss): 249.0681, Val Loss (v_loss): 188.4973\n",
      "Epoch [560/1000], Train Loss (t_loss): 256.6904, Val Loss (v_loss): 213.9041\n",
      "Epoch [561/1000], Train Loss (t_loss): 230.5348, Val Loss (v_loss): 205.9010\n",
      "Epoch [562/1000], Train Loss (t_loss): 232.2524, Val Loss (v_loss): 186.8471\n",
      "Epoch [563/1000], Train Loss (t_loss): 223.7303, Val Loss (v_loss): 144.4034\n",
      "Epoch [564/1000], Train Loss (t_loss): 240.6892, Val Loss (v_loss): 196.9587\n",
      "Epoch [565/1000], Train Loss (t_loss): 223.0441, Val Loss (v_loss): 162.0473\n",
      "Epoch [566/1000], Train Loss (t_loss): 236.6615, Val Loss (v_loss): 137.0680\n",
      "Epoch [567/1000], Train Loss (t_loss): 245.8073, Val Loss (v_loss): 146.2742\n",
      "Epoch [568/1000], Train Loss (t_loss): 238.0687, Val Loss (v_loss): 174.5834\n",
      "Epoch [569/1000], Train Loss (t_loss): 229.7736, Val Loss (v_loss): 159.5302\n",
      "Epoch [570/1000], Train Loss (t_loss): 233.5493, Val Loss (v_loss): 160.6437\n",
      "Epoch [571/1000], Train Loss (t_loss): 246.5379, Val Loss (v_loss): 144.9998\n",
      "Epoch [572/1000], Train Loss (t_loss): 220.7694, Val Loss (v_loss): 162.3902\n",
      "Epoch [573/1000], Train Loss (t_loss): 226.2037, Val Loss (v_loss): 213.4033\n",
      "Epoch [574/1000], Train Loss (t_loss): 241.0667, Val Loss (v_loss): 229.9256\n",
      "Epoch [575/1000], Train Loss (t_loss): 229.0471, Val Loss (v_loss): 199.2747\n",
      "Epoch [576/1000], Train Loss (t_loss): 230.9246, Val Loss (v_loss): 225.3291\n",
      "Epoch [577/1000], Train Loss (t_loss): 235.2652, Val Loss (v_loss): 150.3175\n",
      "Epoch [578/1000], Train Loss (t_loss): 245.8428, Val Loss (v_loss): 164.9259\n",
      "Epoch [579/1000], Train Loss (t_loss): 229.7569, Val Loss (v_loss): 145.9563\n",
      "Epoch [580/1000], Train Loss (t_loss): 230.4212, Val Loss (v_loss): 169.6331\n",
      "Epoch [581/1000], Train Loss (t_loss): 232.6682, Val Loss (v_loss): 197.9054\n",
      "Epoch [582/1000], Train Loss (t_loss): 239.2749, Val Loss (v_loss): 156.9420\n",
      "Epoch [583/1000], Train Loss (t_loss): 230.9254, Val Loss (v_loss): 228.6296\n",
      "Epoch [584/1000], Train Loss (t_loss): 224.7442, Val Loss (v_loss): 135.4167\n",
      "Epoch [585/1000], Train Loss (t_loss): 234.9689, Val Loss (v_loss): 201.0248\n",
      "Epoch [586/1000], Train Loss (t_loss): 209.6719, Val Loss (v_loss): 192.9795\n",
      "Epoch [587/1000], Train Loss (t_loss): 237.0311, Val Loss (v_loss): 136.4067\n",
      "Epoch [588/1000], Train Loss (t_loss): 227.5355, Val Loss (v_loss): 148.2691\n",
      "Epoch [589/1000], Train Loss (t_loss): 225.8266, Val Loss (v_loss): 151.8329\n",
      "Epoch [590/1000], Train Loss (t_loss): 237.2561, Val Loss (v_loss): 164.6368\n",
      "Epoch [591/1000], Train Loss (t_loss): 267.6352, Val Loss (v_loss): 190.2805\n",
      "Epoch [592/1000], Train Loss (t_loss): 235.7802, Val Loss (v_loss): 179.6413\n",
      "Epoch [593/1000], Train Loss (t_loss): 233.9260, Val Loss (v_loss): 155.1302\n",
      "Epoch [594/1000], Train Loss (t_loss): 220.9016, Val Loss (v_loss): 181.6096\n",
      "Epoch [595/1000], Train Loss (t_loss): 226.2016, Val Loss (v_loss): 143.6178\n",
      "Epoch [596/1000], Train Loss (t_loss): 236.4523, Val Loss (v_loss): 186.2418\n",
      "Epoch [597/1000], Train Loss (t_loss): 244.0518, Val Loss (v_loss): 237.5067\n",
      "Epoch [598/1000], Train Loss (t_loss): 231.7019, Val Loss (v_loss): 227.9340\n",
      "Epoch [599/1000], Train Loss (t_loss): 225.0334, Val Loss (v_loss): 146.2149\n",
      "Epoch [600/1000], Train Loss (t_loss): 218.9401, Val Loss (v_loss): 141.7345\n",
      "Epoch [601/1000], Train Loss (t_loss): 238.1906, Val Loss (v_loss): 200.0194\n",
      "Epoch [602/1000], Train Loss (t_loss): 255.8816, Val Loss (v_loss): 194.0408\n",
      "Epoch [603/1000], Train Loss (t_loss): 274.3498, Val Loss (v_loss): 214.2641\n",
      "Epoch [604/1000], Train Loss (t_loss): 218.9060, Val Loss (v_loss): 137.0512\n",
      "Epoch [605/1000], Train Loss (t_loss): 228.0980, Val Loss (v_loss): 163.6461\n",
      "Epoch [606/1000], Train Loss (t_loss): 244.7359, Val Loss (v_loss): 154.9293\n",
      "Epoch [607/1000], Train Loss (t_loss): 232.4693, Val Loss (v_loss): 181.6339\n",
      "Epoch [608/1000], Train Loss (t_loss): 226.1146, Val Loss (v_loss): 153.6922\n",
      "Epoch [609/1000], Train Loss (t_loss): 246.7502, Val Loss (v_loss): 209.2933\n",
      "Epoch [610/1000], Train Loss (t_loss): 229.1016, Val Loss (v_loss): 156.0558\n",
      "Epoch [611/1000], Train Loss (t_loss): 244.4444, Val Loss (v_loss): 257.3485\n",
      "Epoch [612/1000], Train Loss (t_loss): 217.0596, Val Loss (v_loss): 168.6179\n",
      "Epoch [613/1000], Train Loss (t_loss): 247.1570, Val Loss (v_loss): 160.0294\n",
      "Epoch [614/1000], Train Loss (t_loss): 214.5750, Val Loss (v_loss): 157.5440\n",
      "Epoch [615/1000], Train Loss (t_loss): 224.2294, Val Loss (v_loss): 186.7787\n",
      "Epoch [616/1000], Train Loss (t_loss): 231.2783, Val Loss (v_loss): 146.5671\n",
      "Epoch [617/1000], Train Loss (t_loss): 238.6457, Val Loss (v_loss): 145.6903\n",
      "Epoch [618/1000], Train Loss (t_loss): 239.0041, Val Loss (v_loss): 176.5932\n",
      "Epoch [619/1000], Train Loss (t_loss): 238.6424, Val Loss (v_loss): 184.8405\n",
      "Epoch [620/1000], Train Loss (t_loss): 239.2455, Val Loss (v_loss): 188.7237\n",
      "Epoch [621/1000], Train Loss (t_loss): 230.1059, Val Loss (v_loss): 163.2296\n",
      "Epoch [622/1000], Train Loss (t_loss): 231.2505, Val Loss (v_loss): 170.1826\n",
      "Epoch [623/1000], Train Loss (t_loss): 239.0604, Val Loss (v_loss): 135.1562\n",
      "Epoch [624/1000], Train Loss (t_loss): 239.1836, Val Loss (v_loss): 173.0551\n",
      "Epoch [625/1000], Train Loss (t_loss): 262.3727, Val Loss (v_loss): 233.1613\n",
      "Epoch [626/1000], Train Loss (t_loss): 231.5499, Val Loss (v_loss): 150.1676\n",
      "Epoch [627/1000], Train Loss (t_loss): 210.5106, Val Loss (v_loss): 162.6749\n",
      "Epoch [628/1000], Train Loss (t_loss): 243.0594, Val Loss (v_loss): 168.5179\n",
      "Epoch [629/1000], Train Loss (t_loss): 230.9817, Val Loss (v_loss): 174.4938\n",
      "Epoch [630/1000], Train Loss (t_loss): 228.4688, Val Loss (v_loss): 162.6866\n",
      "Epoch [631/1000], Train Loss (t_loss): 241.2376, Val Loss (v_loss): 140.3990\n",
      "Epoch [632/1000], Train Loss (t_loss): 214.3634, Val Loss (v_loss): 152.0806\n",
      "Epoch [633/1000], Train Loss (t_loss): 238.3304, Val Loss (v_loss): 171.8499\n",
      "Epoch [634/1000], Train Loss (t_loss): 229.1837, Val Loss (v_loss): 134.5363\n",
      "Epoch [635/1000], Train Loss (t_loss): 251.2070, Val Loss (v_loss): 136.7145\n",
      "Epoch [636/1000], Train Loss (t_loss): 231.0511, Val Loss (v_loss): 163.6515\n",
      "Epoch [637/1000], Train Loss (t_loss): 219.8910, Val Loss (v_loss): 145.5492\n",
      "Epoch [638/1000], Train Loss (t_loss): 219.2220, Val Loss (v_loss): 186.1180\n",
      "Epoch [639/1000], Train Loss (t_loss): 235.2889, Val Loss (v_loss): 178.8045\n",
      "Epoch [640/1000], Train Loss (t_loss): 229.7311, Val Loss (v_loss): 141.1933\n",
      "Epoch [641/1000], Train Loss (t_loss): 220.5713, Val Loss (v_loss): 225.3004\n",
      "Epoch [642/1000], Train Loss (t_loss): 242.6213, Val Loss (v_loss): 173.2580\n",
      "Epoch [643/1000], Train Loss (t_loss): 225.7026, Val Loss (v_loss): 196.3710\n",
      "Epoch [644/1000], Train Loss (t_loss): 223.6570, Val Loss (v_loss): 241.2805\n",
      "Epoch [645/1000], Train Loss (t_loss): 255.4985, Val Loss (v_loss): 164.3101\n",
      "Epoch [646/1000], Train Loss (t_loss): 218.7781, Val Loss (v_loss): 147.4412\n",
      "Epoch [647/1000], Train Loss (t_loss): 236.6361, Val Loss (v_loss): 162.4339\n",
      "Epoch [648/1000], Train Loss (t_loss): 238.4266, Val Loss (v_loss): 188.3065\n",
      "Epoch [649/1000], Train Loss (t_loss): 222.9836, Val Loss (v_loss): 145.5485\n",
      "Epoch [650/1000], Train Loss (t_loss): 239.2857, Val Loss (v_loss): 169.9069\n",
      "Epoch [651/1000], Train Loss (t_loss): 240.0211, Val Loss (v_loss): 202.1983\n",
      "Epoch [652/1000], Train Loss (t_loss): 224.3904, Val Loss (v_loss): 154.2420\n",
      "Epoch [653/1000], Train Loss (t_loss): 224.5681, Val Loss (v_loss): 184.6454\n",
      "Epoch [654/1000], Train Loss (t_loss): 233.1804, Val Loss (v_loss): 155.6470\n",
      "Epoch [655/1000], Train Loss (t_loss): 232.3527, Val Loss (v_loss): 136.8660\n",
      "Epoch [656/1000], Train Loss (t_loss): 254.3520, Val Loss (v_loss): 144.8543\n",
      "Epoch [657/1000], Train Loss (t_loss): 239.2050, Val Loss (v_loss): 211.2219\n",
      "Epoch [658/1000], Train Loss (t_loss): 270.0057, Val Loss (v_loss): 167.0355\n",
      "Epoch [659/1000], Train Loss (t_loss): 229.8738, Val Loss (v_loss): 172.7789\n",
      "Epoch [660/1000], Train Loss (t_loss): 230.3497, Val Loss (v_loss): 157.3157\n",
      "Epoch [661/1000], Train Loss (t_loss): 219.8609, Val Loss (v_loss): 146.8238\n",
      "Epoch [662/1000], Train Loss (t_loss): 222.9591, Val Loss (v_loss): 202.3258\n",
      "Epoch [663/1000], Train Loss (t_loss): 230.8545, Val Loss (v_loss): 181.7826\n",
      "Epoch [664/1000], Train Loss (t_loss): 212.3791, Val Loss (v_loss): 214.8868\n",
      "Epoch [665/1000], Train Loss (t_loss): 249.8369, Val Loss (v_loss): 143.2108\n",
      "Epoch [666/1000], Train Loss (t_loss): 234.7986, Val Loss (v_loss): 147.6466\n",
      "Epoch [667/1000], Train Loss (t_loss): 237.1675, Val Loss (v_loss): 195.8011\n",
      "Epoch [668/1000], Train Loss (t_loss): 221.4946, Val Loss (v_loss): 239.8838\n",
      "Epoch [669/1000], Train Loss (t_loss): 241.5544, Val Loss (v_loss): 200.3288\n",
      "Epoch [670/1000], Train Loss (t_loss): 219.4529, Val Loss (v_loss): 212.5814\n",
      "Epoch [671/1000], Train Loss (t_loss): 230.8956, Val Loss (v_loss): 165.0985\n",
      "Epoch [672/1000], Train Loss (t_loss): 219.2119, Val Loss (v_loss): 142.5990\n",
      "Epoch [673/1000], Train Loss (t_loss): 234.8333, Val Loss (v_loss): 225.8180\n",
      "Epoch [674/1000], Train Loss (t_loss): 242.6808, Val Loss (v_loss): 145.2096\n",
      "Epoch [675/1000], Train Loss (t_loss): 232.5572, Val Loss (v_loss): 168.8096\n",
      "Epoch [676/1000], Train Loss (t_loss): 246.7613, Val Loss (v_loss): 216.0959\n",
      "Epoch [677/1000], Train Loss (t_loss): 241.6818, Val Loss (v_loss): 272.5323\n",
      "Epoch [678/1000], Train Loss (t_loss): 249.1413, Val Loss (v_loss): 211.2138\n",
      "Epoch [679/1000], Train Loss (t_loss): 225.4724, Val Loss (v_loss): 165.3863\n",
      "Epoch [680/1000], Train Loss (t_loss): 219.4012, Val Loss (v_loss): 207.7602\n",
      "Epoch [681/1000], Train Loss (t_loss): 235.7189, Val Loss (v_loss): 149.2313\n",
      "Epoch [682/1000], Train Loss (t_loss): 220.3521, Val Loss (v_loss): 147.7808\n",
      "Epoch [683/1000], Train Loss (t_loss): 229.3364, Val Loss (v_loss): 185.8391\n",
      "Epoch [684/1000], Train Loss (t_loss): 259.6470, Val Loss (v_loss): 202.1462\n",
      "Epoch [685/1000], Train Loss (t_loss): 235.5989, Val Loss (v_loss): 133.3721\n",
      "Epoch [686/1000], Train Loss (t_loss): 246.3565, Val Loss (v_loss): 136.0098\n",
      "Epoch [687/1000], Train Loss (t_loss): 266.4696, Val Loss (v_loss): 158.2972\n",
      "Epoch [688/1000], Train Loss (t_loss): 254.9911, Val Loss (v_loss): 213.6947\n",
      "Epoch [689/1000], Train Loss (t_loss): 223.2702, Val Loss (v_loss): 167.5454\n",
      "Epoch [690/1000], Train Loss (t_loss): 252.8309, Val Loss (v_loss): 198.0620\n",
      "Epoch [691/1000], Train Loss (t_loss): 220.6082, Val Loss (v_loss): 170.9972\n",
      "Epoch [692/1000], Train Loss (t_loss): 230.0287, Val Loss (v_loss): 185.5569\n",
      "Epoch [693/1000], Train Loss (t_loss): 227.8097, Val Loss (v_loss): 202.3014\n",
      "Epoch [694/1000], Train Loss (t_loss): 218.0345, Val Loss (v_loss): 197.9875\n",
      "Epoch [695/1000], Train Loss (t_loss): 231.2475, Val Loss (v_loss): 149.1693\n",
      "Epoch [696/1000], Train Loss (t_loss): 222.0948, Val Loss (v_loss): 169.0919\n",
      "Epoch [697/1000], Train Loss (t_loss): 230.7901, Val Loss (v_loss): 227.6376\n",
      "Epoch [698/1000], Train Loss (t_loss): 235.1412, Val Loss (v_loss): 161.1867\n",
      "Epoch [699/1000], Train Loss (t_loss): 232.8028, Val Loss (v_loss): 214.0123\n",
      "Epoch [700/1000], Train Loss (t_loss): 226.0340, Val Loss (v_loss): 235.0434\n",
      "Epoch [701/1000], Train Loss (t_loss): 239.8970, Val Loss (v_loss): 190.0621\n",
      "Epoch [702/1000], Train Loss (t_loss): 218.7023, Val Loss (v_loss): 195.2231\n",
      "Epoch [703/1000], Train Loss (t_loss): 233.6973, Val Loss (v_loss): 184.0824\n",
      "Epoch [704/1000], Train Loss (t_loss): 223.9920, Val Loss (v_loss): 185.7871\n",
      "Epoch [705/1000], Train Loss (t_loss): 202.5912, Val Loss (v_loss): 135.4490\n",
      "Epoch [706/1000], Train Loss (t_loss): 260.6729, Val Loss (v_loss): 181.1233\n",
      "Epoch [707/1000], Train Loss (t_loss): 228.2266, Val Loss (v_loss): 191.5461\n",
      "Epoch [708/1000], Train Loss (t_loss): 205.5072, Val Loss (v_loss): 139.8445\n",
      "Epoch [709/1000], Train Loss (t_loss): 231.9160, Val Loss (v_loss): 155.3133\n",
      "Epoch [710/1000], Train Loss (t_loss): 223.4311, Val Loss (v_loss): 296.2182\n",
      "Epoch [711/1000], Train Loss (t_loss): 245.2230, Val Loss (v_loss): 167.1593\n",
      "Epoch [712/1000], Train Loss (t_loss): 242.6752, Val Loss (v_loss): 136.4001\n",
      "Epoch [713/1000], Train Loss (t_loss): 231.9399, Val Loss (v_loss): 151.3741\n",
      "Epoch [714/1000], Train Loss (t_loss): 221.8692, Val Loss (v_loss): 165.9190\n",
      "Epoch [715/1000], Train Loss (t_loss): 244.4881, Val Loss (v_loss): 151.5990\n",
      "Epoch [716/1000], Train Loss (t_loss): 236.6697, Val Loss (v_loss): 146.8305\n",
      "Epoch [717/1000], Train Loss (t_loss): 237.5632, Val Loss (v_loss): 206.6583\n",
      "Epoch [718/1000], Train Loss (t_loss): 215.8341, Val Loss (v_loss): 177.2698\n",
      "Epoch [719/1000], Train Loss (t_loss): 218.1281, Val Loss (v_loss): 163.2657\n",
      "Epoch [720/1000], Train Loss (t_loss): 225.5261, Val Loss (v_loss): 189.9665\n",
      "Epoch [721/1000], Train Loss (t_loss): 232.7358, Val Loss (v_loss): 197.7967\n",
      "Epoch [722/1000], Train Loss (t_loss): 224.3707, Val Loss (v_loss): 207.4984\n",
      "Epoch [723/1000], Train Loss (t_loss): 248.4044, Val Loss (v_loss): 189.5872\n",
      "Epoch [724/1000], Train Loss (t_loss): 217.2008, Val Loss (v_loss): 156.4999\n",
      "Epoch [725/1000], Train Loss (t_loss): 215.1737, Val Loss (v_loss): 145.3927\n",
      "Epoch [726/1000], Train Loss (t_loss): 214.9217, Val Loss (v_loss): 198.8265\n",
      "Epoch [727/1000], Train Loss (t_loss): 222.3570, Val Loss (v_loss): 160.1931\n",
      "Epoch [728/1000], Train Loss (t_loss): 227.6690, Val Loss (v_loss): 156.5954\n",
      "Epoch [729/1000], Train Loss (t_loss): 244.2825, Val Loss (v_loss): 149.7373\n",
      "Epoch [730/1000], Train Loss (t_loss): 218.1198, Val Loss (v_loss): 202.9779\n",
      "Epoch [731/1000], Train Loss (t_loss): 231.1664, Val Loss (v_loss): 202.9201\n",
      "Epoch [732/1000], Train Loss (t_loss): 234.0351, Val Loss (v_loss): 169.5063\n",
      "Epoch [733/1000], Train Loss (t_loss): 240.8898, Val Loss (v_loss): 204.8742\n",
      "Epoch [734/1000], Train Loss (t_loss): 229.5161, Val Loss (v_loss): 156.0322\n",
      "Epoch [735/1000], Train Loss (t_loss): 202.6606, Val Loss (v_loss): 148.1189\n",
      "Epoch [736/1000], Train Loss (t_loss): 214.9254, Val Loss (v_loss): 186.3773\n",
      "Epoch [737/1000], Train Loss (t_loss): 222.9830, Val Loss (v_loss): 161.0394\n",
      "Epoch [738/1000], Train Loss (t_loss): 234.6765, Val Loss (v_loss): 208.4135\n",
      "Epoch [739/1000], Train Loss (t_loss): 208.8795, Val Loss (v_loss): 129.8474\n",
      "Epoch [740/1000], Train Loss (t_loss): 235.4733, Val Loss (v_loss): 145.9704\n",
      "Epoch [741/1000], Train Loss (t_loss): 218.0842, Val Loss (v_loss): 141.7822\n",
      "Epoch [742/1000], Train Loss (t_loss): 223.5465, Val Loss (v_loss): 140.9262\n",
      "Epoch [743/1000], Train Loss (t_loss): 222.0273, Val Loss (v_loss): 199.3892\n",
      "Epoch [744/1000], Train Loss (t_loss): 227.3286, Val Loss (v_loss): 198.1448\n",
      "Epoch [745/1000], Train Loss (t_loss): 231.2982, Val Loss (v_loss): 152.2266\n",
      "Epoch [746/1000], Train Loss (t_loss): 238.1165, Val Loss (v_loss): 178.6742\n",
      "Epoch [747/1000], Train Loss (t_loss): 225.5544, Val Loss (v_loss): 144.5767\n",
      "Epoch [748/1000], Train Loss (t_loss): 240.1863, Val Loss (v_loss): 177.9923\n",
      "Epoch [749/1000], Train Loss (t_loss): 232.2219, Val Loss (v_loss): 143.6376\n",
      "Epoch [750/1000], Train Loss (t_loss): 210.0334, Val Loss (v_loss): 155.8255\n",
      "Epoch [751/1000], Train Loss (t_loss): 216.8494, Val Loss (v_loss): 160.5069\n",
      "Epoch [752/1000], Train Loss (t_loss): 235.5267, Val Loss (v_loss): 234.0546\n",
      "Epoch [753/1000], Train Loss (t_loss): 233.0449, Val Loss (v_loss): 244.0448\n",
      "Epoch [754/1000], Train Loss (t_loss): 258.9029, Val Loss (v_loss): 155.9407\n",
      "Epoch [755/1000], Train Loss (t_loss): 224.3745, Val Loss (v_loss): 154.3289\n",
      "Epoch [756/1000], Train Loss (t_loss): 236.8131, Val Loss (v_loss): 155.6965\n",
      "Epoch [757/1000], Train Loss (t_loss): 227.3977, Val Loss (v_loss): 177.0021\n",
      "Epoch [758/1000], Train Loss (t_loss): 217.5318, Val Loss (v_loss): 177.4614\n",
      "Epoch [759/1000], Train Loss (t_loss): 227.2151, Val Loss (v_loss): 193.3630\n",
      "Epoch [760/1000], Train Loss (t_loss): 222.0500, Val Loss (v_loss): 181.8039\n",
      "Epoch [761/1000], Train Loss (t_loss): 212.5857, Val Loss (v_loss): 168.4246\n",
      "Epoch [762/1000], Train Loss (t_loss): 236.7999, Val Loss (v_loss): 195.3862\n",
      "Epoch [763/1000], Train Loss (t_loss): 242.8150, Val Loss (v_loss): 180.4635\n",
      "Epoch [764/1000], Train Loss (t_loss): 237.2200, Val Loss (v_loss): 142.9811\n",
      "Epoch [765/1000], Train Loss (t_loss): 213.8715, Val Loss (v_loss): 164.8412\n",
      "Epoch [766/1000], Train Loss (t_loss): 219.9553, Val Loss (v_loss): 239.8796\n",
      "Epoch [767/1000], Train Loss (t_loss): 247.3363, Val Loss (v_loss): 191.8111\n",
      "Epoch [768/1000], Train Loss (t_loss): 216.3075, Val Loss (v_loss): 147.5711\n",
      "Epoch [769/1000], Train Loss (t_loss): 209.5410, Val Loss (v_loss): 159.1888\n",
      "Epoch [770/1000], Train Loss (t_loss): 225.2533, Val Loss (v_loss): 193.0682\n",
      "Epoch [771/1000], Train Loss (t_loss): 214.8791, Val Loss (v_loss): 176.4197\n",
      "Epoch [772/1000], Train Loss (t_loss): 226.0982, Val Loss (v_loss): 171.5946\n",
      "Epoch [773/1000], Train Loss (t_loss): 206.0027, Val Loss (v_loss): 140.4468\n",
      "Epoch [774/1000], Train Loss (t_loss): 224.4782, Val Loss (v_loss): 149.7403\n",
      "Epoch [775/1000], Train Loss (t_loss): 234.5353, Val Loss (v_loss): 140.0979\n",
      "Epoch [776/1000], Train Loss (t_loss): 229.2885, Val Loss (v_loss): 207.3560\n",
      "Epoch [777/1000], Train Loss (t_loss): 223.7489, Val Loss (v_loss): 229.7521\n",
      "Epoch [778/1000], Train Loss (t_loss): 217.7920, Val Loss (v_loss): 246.9485\n",
      "Epoch [779/1000], Train Loss (t_loss): 238.4310, Val Loss (v_loss): 180.7694\n",
      "Epoch [780/1000], Train Loss (t_loss): 195.3217, Val Loss (v_loss): 155.7338\n",
      "Epoch [781/1000], Train Loss (t_loss): 222.0706, Val Loss (v_loss): 170.8412\n",
      "Epoch [782/1000], Train Loss (t_loss): 210.7782, Val Loss (v_loss): 228.0106\n",
      "Epoch [783/1000], Train Loss (t_loss): 229.6070, Val Loss (v_loss): 166.9757\n",
      "Epoch [784/1000], Train Loss (t_loss): 212.3101, Val Loss (v_loss): 196.3664\n",
      "Epoch [785/1000], Train Loss (t_loss): 231.5822, Val Loss (v_loss): 181.4848\n",
      "Epoch [786/1000], Train Loss (t_loss): 212.8858, Val Loss (v_loss): 147.9790\n",
      "Epoch [787/1000], Train Loss (t_loss): 226.5635, Val Loss (v_loss): 184.8872\n",
      "Epoch [788/1000], Train Loss (t_loss): 212.9344, Val Loss (v_loss): 181.2154\n",
      "Epoch [789/1000], Train Loss (t_loss): 218.6576, Val Loss (v_loss): 175.5622\n",
      "Epoch [790/1000], Train Loss (t_loss): 225.9207, Val Loss (v_loss): 179.4250\n",
      "Epoch [791/1000], Train Loss (t_loss): 238.7624, Val Loss (v_loss): 187.3400\n",
      "Epoch [792/1000], Train Loss (t_loss): 246.0679, Val Loss (v_loss): 162.3109\n",
      "Epoch [793/1000], Train Loss (t_loss): 216.9715, Val Loss (v_loss): 150.7161\n",
      "Epoch [794/1000], Train Loss (t_loss): 226.9391, Val Loss (v_loss): 172.9697\n",
      "Epoch [795/1000], Train Loss (t_loss): 238.6666, Val Loss (v_loss): 183.1254\n",
      "Epoch [796/1000], Train Loss (t_loss): 227.0398, Val Loss (v_loss): 185.7501\n",
      "Epoch [797/1000], Train Loss (t_loss): 247.9770, Val Loss (v_loss): 203.8945\n",
      "Epoch [798/1000], Train Loss (t_loss): 225.9712, Val Loss (v_loss): 189.4928\n",
      "Epoch [799/1000], Train Loss (t_loss): 218.7607, Val Loss (v_loss): 186.9482\n",
      "Epoch [800/1000], Train Loss (t_loss): 220.5658, Val Loss (v_loss): 190.0885\n",
      "Epoch [801/1000], Train Loss (t_loss): 214.5643, Val Loss (v_loss): 181.5426\n",
      "Epoch [802/1000], Train Loss (t_loss): 220.9077, Val Loss (v_loss): 155.4902\n",
      "Epoch [803/1000], Train Loss (t_loss): 230.7042, Val Loss (v_loss): 194.7276\n",
      "Epoch [804/1000], Train Loss (t_loss): 233.7659, Val Loss (v_loss): 174.0925\n",
      "Epoch [805/1000], Train Loss (t_loss): 204.9360, Val Loss (v_loss): 192.1564\n",
      "Epoch [806/1000], Train Loss (t_loss): 225.5021, Val Loss (v_loss): 165.6123\n",
      "Epoch [807/1000], Train Loss (t_loss): 226.3509, Val Loss (v_loss): 172.8738\n",
      "Epoch [808/1000], Train Loss (t_loss): 220.7778, Val Loss (v_loss): 182.7127\n",
      "Epoch [809/1000], Train Loss (t_loss): 222.1342, Val Loss (v_loss): 168.3555\n",
      "Epoch [810/1000], Train Loss (t_loss): 216.0272, Val Loss (v_loss): 158.9647\n",
      "Epoch [811/1000], Train Loss (t_loss): 233.8559, Val Loss (v_loss): 153.5499\n",
      "Epoch [812/1000], Train Loss (t_loss): 231.9084, Val Loss (v_loss): 184.8757\n",
      "Epoch [813/1000], Train Loss (t_loss): 228.3099, Val Loss (v_loss): 184.6151\n",
      "Epoch [814/1000], Train Loss (t_loss): 217.2465, Val Loss (v_loss): 178.8305\n",
      "Epoch [815/1000], Train Loss (t_loss): 204.1678, Val Loss (v_loss): 183.4918\n",
      "Epoch [816/1000], Train Loss (t_loss): 225.8891, Val Loss (v_loss): 181.7265\n",
      "Epoch [817/1000], Train Loss (t_loss): 239.8385, Val Loss (v_loss): 235.1797\n",
      "Epoch [818/1000], Train Loss (t_loss): 222.8782, Val Loss (v_loss): 165.0149\n",
      "Epoch [819/1000], Train Loss (t_loss): 226.9265, Val Loss (v_loss): 197.2762\n",
      "Epoch [820/1000], Train Loss (t_loss): 212.6499, Val Loss (v_loss): 222.6508\n",
      "Epoch [821/1000], Train Loss (t_loss): 220.6826, Val Loss (v_loss): 180.8747\n",
      "Epoch [822/1000], Train Loss (t_loss): 210.3626, Val Loss (v_loss): 215.4128\n",
      "Epoch [823/1000], Train Loss (t_loss): 239.5298, Val Loss (v_loss): 194.5361\n",
      "Epoch [824/1000], Train Loss (t_loss): 229.9004, Val Loss (v_loss): 152.8493\n",
      "Epoch [825/1000], Train Loss (t_loss): 235.7803, Val Loss (v_loss): 172.3066\n",
      "Epoch [826/1000], Train Loss (t_loss): 221.7414, Val Loss (v_loss): 248.2010\n",
      "Epoch [827/1000], Train Loss (t_loss): 214.2619, Val Loss (v_loss): 179.2760\n",
      "Epoch [828/1000], Train Loss (t_loss): 226.7367, Val Loss (v_loss): 193.0423\n",
      "Epoch [829/1000], Train Loss (t_loss): 237.4326, Val Loss (v_loss): 259.7246\n",
      "Epoch [830/1000], Train Loss (t_loss): 235.8430, Val Loss (v_loss): 182.9066\n",
      "Epoch [831/1000], Train Loss (t_loss): 240.0565, Val Loss (v_loss): 139.8262\n",
      "Epoch [832/1000], Train Loss (t_loss): 247.8706, Val Loss (v_loss): 219.5801\n",
      "Epoch [833/1000], Train Loss (t_loss): 231.3982, Val Loss (v_loss): 262.4735\n",
      "Epoch [834/1000], Train Loss (t_loss): 227.7375, Val Loss (v_loss): 230.7677\n",
      "Epoch [835/1000], Train Loss (t_loss): 216.1741, Val Loss (v_loss): 187.0320\n",
      "Epoch [836/1000], Train Loss (t_loss): 222.8345, Val Loss (v_loss): 228.5562\n",
      "Epoch [837/1000], Train Loss (t_loss): 238.7243, Val Loss (v_loss): 194.5235\n",
      "Epoch [838/1000], Train Loss (t_loss): 227.0597, Val Loss (v_loss): 173.8328\n",
      "Epoch [839/1000], Train Loss (t_loss): 219.7657, Val Loss (v_loss): 215.1632\n",
      "Epoch [840/1000], Train Loss (t_loss): 222.9044, Val Loss (v_loss): 172.6865\n",
      "Epoch [841/1000], Train Loss (t_loss): 196.1182, Val Loss (v_loss): 203.7617\n",
      "Epoch [842/1000], Train Loss (t_loss): 210.3940, Val Loss (v_loss): 195.6453\n",
      "Epoch [843/1000], Train Loss (t_loss): 221.5467, Val Loss (v_loss): 262.2136\n",
      "Epoch [844/1000], Train Loss (t_loss): 214.2305, Val Loss (v_loss): 167.3938\n",
      "Epoch [845/1000], Train Loss (t_loss): 216.3123, Val Loss (v_loss): 157.2854\n",
      "Epoch [846/1000], Train Loss (t_loss): 238.0948, Val Loss (v_loss): 186.5969\n",
      "Epoch [847/1000], Train Loss (t_loss): 215.2176, Val Loss (v_loss): 145.1117\n",
      "Epoch [848/1000], Train Loss (t_loss): 224.0136, Val Loss (v_loss): 141.1575\n",
      "Epoch [849/1000], Train Loss (t_loss): 234.9346, Val Loss (v_loss): 167.1827\n",
      "Epoch [850/1000], Train Loss (t_loss): 216.4853, Val Loss (v_loss): 164.5864\n",
      "Epoch [851/1000], Train Loss (t_loss): 219.0279, Val Loss (v_loss): 183.2662\n",
      "Epoch [852/1000], Train Loss (t_loss): 238.8785, Val Loss (v_loss): 211.8783\n",
      "Epoch [853/1000], Train Loss (t_loss): 239.7126, Val Loss (v_loss): 187.1754\n",
      "Epoch [854/1000], Train Loss (t_loss): 225.7056, Val Loss (v_loss): 153.4450\n",
      "Epoch [855/1000], Train Loss (t_loss): 242.5556, Val Loss (v_loss): 190.5180\n",
      "Epoch [856/1000], Train Loss (t_loss): 214.7189, Val Loss (v_loss): 156.6550\n",
      "Epoch [857/1000], Train Loss (t_loss): 221.1310, Val Loss (v_loss): 163.5615\n",
      "Epoch [858/1000], Train Loss (t_loss): 200.1770, Val Loss (v_loss): 177.4840\n",
      "Epoch [859/1000], Train Loss (t_loss): 223.4573, Val Loss (v_loss): 146.9256\n",
      "Epoch [860/1000], Train Loss (t_loss): 234.1695, Val Loss (v_loss): 138.1375\n",
      "Epoch [861/1000], Train Loss (t_loss): 231.5733, Val Loss (v_loss): 185.7897\n",
      "Epoch [862/1000], Train Loss (t_loss): 207.8588, Val Loss (v_loss): 169.6549\n",
      "Epoch [863/1000], Train Loss (t_loss): 234.0866, Val Loss (v_loss): 159.7063\n",
      "Epoch [864/1000], Train Loss (t_loss): 213.1165, Val Loss (v_loss): 152.4605\n",
      "Epoch [865/1000], Train Loss (t_loss): 214.9226, Val Loss (v_loss): 171.4656\n",
      "Epoch [866/1000], Train Loss (t_loss): 224.3453, Val Loss (v_loss): 185.0130\n",
      "Epoch [867/1000], Train Loss (t_loss): 224.8682, Val Loss (v_loss): 225.7113\n",
      "Epoch [868/1000], Train Loss (t_loss): 220.2187, Val Loss (v_loss): 188.1575\n",
      "Epoch [869/1000], Train Loss (t_loss): 222.0191, Val Loss (v_loss): 193.9164\n",
      "Epoch [870/1000], Train Loss (t_loss): 223.6593, Val Loss (v_loss): 173.2734\n",
      "Epoch [871/1000], Train Loss (t_loss): 210.8502, Val Loss (v_loss): 152.4120\n",
      "Epoch [872/1000], Train Loss (t_loss): 225.9471, Val Loss (v_loss): 211.4457\n",
      "Epoch [873/1000], Train Loss (t_loss): 228.3357, Val Loss (v_loss): 193.7321\n",
      "Epoch [874/1000], Train Loss (t_loss): 220.8774, Val Loss (v_loss): 205.2962\n",
      "Epoch [875/1000], Train Loss (t_loss): 206.5252, Val Loss (v_loss): 168.0418\n",
      "Epoch [876/1000], Train Loss (t_loss): 211.9006, Val Loss (v_loss): 187.0428\n",
      "Epoch [877/1000], Train Loss (t_loss): 248.3319, Val Loss (v_loss): 315.8001\n",
      "Epoch [878/1000], Train Loss (t_loss): 228.8960, Val Loss (v_loss): 202.3918\n",
      "Epoch [879/1000], Train Loss (t_loss): 223.9648, Val Loss (v_loss): 200.0415\n",
      "Epoch [880/1000], Train Loss (t_loss): 215.0592, Val Loss (v_loss): 192.8642\n",
      "Epoch [881/1000], Train Loss (t_loss): 223.6211, Val Loss (v_loss): 162.2188\n",
      "Epoch [882/1000], Train Loss (t_loss): 219.0568, Val Loss (v_loss): 202.0067\n",
      "Epoch [883/1000], Train Loss (t_loss): 226.8529, Val Loss (v_loss): 192.3663\n",
      "Epoch [884/1000], Train Loss (t_loss): 198.9903, Val Loss (v_loss): 253.0423\n",
      "Epoch [885/1000], Train Loss (t_loss): 228.5820, Val Loss (v_loss): 193.7857\n",
      "Epoch [886/1000], Train Loss (t_loss): 206.2460, Val Loss (v_loss): 155.9542\n",
      "Epoch [887/1000], Train Loss (t_loss): 233.9981, Val Loss (v_loss): 159.2015\n",
      "Epoch [888/1000], Train Loss (t_loss): 207.3174, Val Loss (v_loss): 194.4331\n",
      "Epoch [889/1000], Train Loss (t_loss): 216.3046, Val Loss (v_loss): 144.0833\n",
      "Epoch [890/1000], Train Loss (t_loss): 205.6558, Val Loss (v_loss): 195.2233\n",
      "Epoch [891/1000], Train Loss (t_loss): 219.6336, Val Loss (v_loss): 171.3012\n",
      "Epoch [892/1000], Train Loss (t_loss): 214.1607, Val Loss (v_loss): 139.1061\n",
      "Epoch [893/1000], Train Loss (t_loss): 226.3994, Val Loss (v_loss): 168.9432\n",
      "Epoch [894/1000], Train Loss (t_loss): 233.0768, Val Loss (v_loss): 165.8883\n",
      "Epoch [895/1000], Train Loss (t_loss): 221.5005, Val Loss (v_loss): 169.5085\n",
      "Epoch [896/1000], Train Loss (t_loss): 225.8741, Val Loss (v_loss): 186.5376\n",
      "Epoch [897/1000], Train Loss (t_loss): 212.5579, Val Loss (v_loss): 192.7745\n",
      "Epoch [898/1000], Train Loss (t_loss): 219.2160, Val Loss (v_loss): 180.6435\n",
      "Epoch [899/1000], Train Loss (t_loss): 213.6420, Val Loss (v_loss): 135.4476\n",
      "Epoch [900/1000], Train Loss (t_loss): 204.6546, Val Loss (v_loss): 166.2149\n",
      "Epoch [901/1000], Train Loss (t_loss): 231.8840, Val Loss (v_loss): 226.4968\n",
      "Epoch [902/1000], Train Loss (t_loss): 224.6713, Val Loss (v_loss): 200.5473\n",
      "Epoch [903/1000], Train Loss (t_loss): 226.4255, Val Loss (v_loss): 158.4427\n",
      "Epoch [904/1000], Train Loss (t_loss): 197.0466, Val Loss (v_loss): 152.0320\n",
      "Epoch [905/1000], Train Loss (t_loss): 231.3683, Val Loss (v_loss): 201.2815\n",
      "Epoch [906/1000], Train Loss (t_loss): 214.5020, Val Loss (v_loss): 210.0465\n",
      "Epoch [907/1000], Train Loss (t_loss): 225.3291, Val Loss (v_loss): 186.2611\n",
      "Epoch [908/1000], Train Loss (t_loss): 226.0308, Val Loss (v_loss): 155.9408\n",
      "Epoch [909/1000], Train Loss (t_loss): 227.2435, Val Loss (v_loss): 181.9914\n",
      "Epoch [910/1000], Train Loss (t_loss): 223.8094, Val Loss (v_loss): 151.0544\n",
      "Epoch [911/1000], Train Loss (t_loss): 216.0221, Val Loss (v_loss): 152.0468\n",
      "Epoch [912/1000], Train Loss (t_loss): 221.7770, Val Loss (v_loss): 157.1117\n",
      "Epoch [913/1000], Train Loss (t_loss): 219.6315, Val Loss (v_loss): 155.8415\n",
      "Epoch [914/1000], Train Loss (t_loss): 225.8401, Val Loss (v_loss): 158.2009\n",
      "Epoch [915/1000], Train Loss (t_loss): 236.3369, Val Loss (v_loss): 139.5910\n",
      "Epoch [916/1000], Train Loss (t_loss): 216.1519, Val Loss (v_loss): 167.5638\n",
      "Epoch [917/1000], Train Loss (t_loss): 210.3105, Val Loss (v_loss): 164.3624\n",
      "Epoch [918/1000], Train Loss (t_loss): 210.3745, Val Loss (v_loss): 194.7231\n",
      "Epoch [919/1000], Train Loss (t_loss): 219.8914, Val Loss (v_loss): 187.2176\n",
      "Epoch [920/1000], Train Loss (t_loss): 214.7369, Val Loss (v_loss): 230.4841\n",
      "Epoch [921/1000], Train Loss (t_loss): 208.8180, Val Loss (v_loss): 179.9401\n",
      "Epoch [922/1000], Train Loss (t_loss): 210.2402, Val Loss (v_loss): 201.4416\n",
      "Epoch [923/1000], Train Loss (t_loss): 229.6060, Val Loss (v_loss): 191.5422\n",
      "Epoch [924/1000], Train Loss (t_loss): 211.9704, Val Loss (v_loss): 148.0648\n",
      "Epoch [925/1000], Train Loss (t_loss): 218.6035, Val Loss (v_loss): 186.1352\n",
      "Epoch [926/1000], Train Loss (t_loss): 214.3771, Val Loss (v_loss): 184.2015\n",
      "Epoch [927/1000], Train Loss (t_loss): 212.3068, Val Loss (v_loss): 218.4320\n",
      "Epoch [928/1000], Train Loss (t_loss): 229.2843, Val Loss (v_loss): 188.9902\n",
      "Epoch [929/1000], Train Loss (t_loss): 232.1008, Val Loss (v_loss): 207.8170\n",
      "Epoch [930/1000], Train Loss (t_loss): 212.9192, Val Loss (v_loss): 157.9444\n",
      "Epoch [931/1000], Train Loss (t_loss): 224.7793, Val Loss (v_loss): 201.3389\n",
      "Epoch [932/1000], Train Loss (t_loss): 218.9944, Val Loss (v_loss): 197.2696\n",
      "Epoch [933/1000], Train Loss (t_loss): 205.7725, Val Loss (v_loss): 205.6037\n",
      "Epoch [934/1000], Train Loss (t_loss): 233.5608, Val Loss (v_loss): 200.5399\n",
      "Epoch [935/1000], Train Loss (t_loss): 211.5670, Val Loss (v_loss): 178.4528\n",
      "Epoch [936/1000], Train Loss (t_loss): 201.7564, Val Loss (v_loss): 157.9223\n",
      "Epoch [937/1000], Train Loss (t_loss): 226.6496, Val Loss (v_loss): 177.7013\n",
      "Epoch [938/1000], Train Loss (t_loss): 233.9538, Val Loss (v_loss): 160.6668\n",
      "Epoch [939/1000], Train Loss (t_loss): 208.7102, Val Loss (v_loss): 155.5586\n",
      "Epoch [940/1000], Train Loss (t_loss): 210.3271, Val Loss (v_loss): 197.0011\n",
      "Epoch [941/1000], Train Loss (t_loss): 206.7587, Val Loss (v_loss): 179.2088\n",
      "Epoch [942/1000], Train Loss (t_loss): 207.8290, Val Loss (v_loss): 192.8130\n",
      "Epoch [943/1000], Train Loss (t_loss): 229.7502, Val Loss (v_loss): 195.7907\n",
      "Epoch [944/1000], Train Loss (t_loss): 231.3443, Val Loss (v_loss): 190.6224\n",
      "Epoch [945/1000], Train Loss (t_loss): 199.8739, Val Loss (v_loss): 163.5984\n",
      "Epoch [946/1000], Train Loss (t_loss): 213.0651, Val Loss (v_loss): 192.6869\n",
      "Epoch [947/1000], Train Loss (t_loss): 219.1076, Val Loss (v_loss): 185.3752\n",
      "Epoch [948/1000], Train Loss (t_loss): 229.7749, Val Loss (v_loss): 215.2001\n",
      "Epoch [949/1000], Train Loss (t_loss): 211.7998, Val Loss (v_loss): 145.4530\n",
      "Epoch [950/1000], Train Loss (t_loss): 218.5650, Val Loss (v_loss): 156.7090\n",
      "Epoch [951/1000], Train Loss (t_loss): 200.9659, Val Loss (v_loss): 174.5709\n",
      "Epoch [952/1000], Train Loss (t_loss): 225.2569, Val Loss (v_loss): 178.5427\n",
      "Epoch [953/1000], Train Loss (t_loss): 215.0050, Val Loss (v_loss): 179.7614\n",
      "Epoch [954/1000], Train Loss (t_loss): 222.7272, Val Loss (v_loss): 173.6832\n",
      "Epoch [955/1000], Train Loss (t_loss): 209.0483, Val Loss (v_loss): 199.3531\n",
      "Epoch [956/1000], Train Loss (t_loss): 208.3080, Val Loss (v_loss): 169.5697\n",
      "Epoch [957/1000], Train Loss (t_loss): 192.8119, Val Loss (v_loss): 154.5658\n",
      "Epoch [958/1000], Train Loss (t_loss): 216.6913, Val Loss (v_loss): 179.5956\n",
      "Epoch [959/1000], Train Loss (t_loss): 213.4761, Val Loss (v_loss): 158.4894\n",
      "Epoch [960/1000], Train Loss (t_loss): 224.6477, Val Loss (v_loss): 158.3958\n",
      "Epoch [961/1000], Train Loss (t_loss): 204.6348, Val Loss (v_loss): 183.7253\n",
      "Epoch [962/1000], Train Loss (t_loss): 206.9966, Val Loss (v_loss): 201.8608\n",
      "Epoch [963/1000], Train Loss (t_loss): 222.4100, Val Loss (v_loss): 191.2035\n",
      "Epoch [964/1000], Train Loss (t_loss): 215.9750, Val Loss (v_loss): 185.5908\n",
      "Epoch [965/1000], Train Loss (t_loss): 197.7525, Val Loss (v_loss): 181.1492\n",
      "Epoch [966/1000], Train Loss (t_loss): 217.5268, Val Loss (v_loss): 186.6765\n",
      "Epoch [967/1000], Train Loss (t_loss): 222.1414, Val Loss (v_loss): 217.0796\n",
      "Epoch [968/1000], Train Loss (t_loss): 197.5273, Val Loss (v_loss): 232.1076\n",
      "Epoch [969/1000], Train Loss (t_loss): 238.9631, Val Loss (v_loss): 158.7048\n",
      "Epoch [970/1000], Train Loss (t_loss): 195.1526, Val Loss (v_loss): 158.5972\n",
      "Epoch [971/1000], Train Loss (t_loss): 211.3837, Val Loss (v_loss): 139.0443\n",
      "Epoch [972/1000], Train Loss (t_loss): 212.9620, Val Loss (v_loss): 173.3842\n",
      "Epoch [973/1000], Train Loss (t_loss): 229.6211, Val Loss (v_loss): 229.8217\n",
      "Epoch [974/1000], Train Loss (t_loss): 232.9961, Val Loss (v_loss): 180.4052\n",
      "Epoch [975/1000], Train Loss (t_loss): 206.7110, Val Loss (v_loss): 167.0476\n",
      "Epoch [976/1000], Train Loss (t_loss): 215.6566, Val Loss (v_loss): 159.4028\n",
      "Epoch [977/1000], Train Loss (t_loss): 221.9385, Val Loss (v_loss): 157.7187\n",
      "Epoch [978/1000], Train Loss (t_loss): 247.4076, Val Loss (v_loss): 173.1249\n",
      "Epoch [979/1000], Train Loss (t_loss): 219.7126, Val Loss (v_loss): 151.3241\n",
      "Epoch [980/1000], Train Loss (t_loss): 230.5147, Val Loss (v_loss): 142.3109\n",
      "Epoch [981/1000], Train Loss (t_loss): 209.1926, Val Loss (v_loss): 197.0513\n",
      "Epoch [982/1000], Train Loss (t_loss): 205.3498, Val Loss (v_loss): 195.9629\n",
      "Epoch [983/1000], Train Loss (t_loss): 194.0740, Val Loss (v_loss): 157.5919\n",
      "Epoch [984/1000], Train Loss (t_loss): 224.7599, Val Loss (v_loss): 149.4911\n",
      "Epoch [985/1000], Train Loss (t_loss): 221.1394, Val Loss (v_loss): 153.4563\n",
      "Epoch [986/1000], Train Loss (t_loss): 228.2719, Val Loss (v_loss): 185.1756\n",
      "Epoch [987/1000], Train Loss (t_loss): 204.4621, Val Loss (v_loss): 192.1697\n",
      "Epoch [988/1000], Train Loss (t_loss): 211.2943, Val Loss (v_loss): 191.7149\n",
      "Epoch [989/1000], Train Loss (t_loss): 233.9556, Val Loss (v_loss): 186.4074\n",
      "Epoch [990/1000], Train Loss (t_loss): 230.0747, Val Loss (v_loss): 190.0487\n",
      "Epoch [991/1000], Train Loss (t_loss): 211.6506, Val Loss (v_loss): 183.2146\n",
      "Epoch [992/1000], Train Loss (t_loss): 221.9391, Val Loss (v_loss): 185.3949\n",
      "Epoch [993/1000], Train Loss (t_loss): 225.2393, Val Loss (v_loss): 150.0614\n",
      "Epoch [994/1000], Train Loss (t_loss): 209.5440, Val Loss (v_loss): 170.0398\n",
      "Epoch [995/1000], Train Loss (t_loss): 215.9258, Val Loss (v_loss): 167.6899\n",
      "Epoch [996/1000], Train Loss (t_loss): 212.3472, Val Loss (v_loss): 184.9547\n",
      "Epoch [997/1000], Train Loss (t_loss): 218.2187, Val Loss (v_loss): 187.0172\n",
      "Epoch [998/1000], Train Loss (t_loss): 202.9494, Val Loss (v_loss): 192.1918\n",
      "Epoch [999/1000], Train Loss (t_loss): 221.9306, Val Loss (v_loss): 195.2673\n",
      "Epoch [1000/1000], Train Loss (t_loss): 210.3970, Val Loss (v_loss): 151.5247\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mae_criterion = nn.L1Loss()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Perhaps should use a validation set, but for now will use test set for validation\n",
    "# Only using test set for monitoring of overfitting, model isn't actively changing based on it\n",
    "\n",
    "trainLoss = []\n",
    "validationLoss = []\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * targets.size(0)\n",
    "    \n",
    "    avg_train_loss = running_loss / len(X_train)\n",
    "    trainLoss.append(avg_train_loss) # Store the average training loss\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    val_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            val_loss = criterion(outputs, targets) # Use the same loss for comparison\n",
    "            running_val_loss += val_loss.item() * targets.size(0)\n",
    "            val_samples += targets.size(0)\n",
    "    \n",
    "    avg_val_loss = running_val_loss / val_samples\n",
    "    validationLoss.append(avg_val_loss) # Store the average validation loss\n",
    "    print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Train Loss (t_loss): {avg_train_loss:.4f}, Val Loss (v_loss): {avg_val_loss:.4f}')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(trainLoss[5:], label='Training Loss (t_loss)', color='blue')\n",
    "plt.plot(validationLoss[5:], label='Validation/Test Loss (v_loss)', color='red')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show() #\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8603a23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Evaluation on Test Set ---\n",
      "R:\n",
      "[0.07353408698498365, 0.6910243895991995, 0.2720302300641704]\n",
      "\n",
      "--- Test Set Metrics ---\n",
      "MSE:  151.524666\n",
      "RMSE: 12.309536\n",
      "MAE:  8.256471\n",
      "\n",
      "Training and Evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Final Evaluation on Test Set ---\")\n",
    "model.eval()\n",
    "\n",
    "total_loss = 0.0\n",
    "total_samples = 0\n",
    "all_targets = []\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        total_loss += loss.item() * targets.size(0)\n",
    "        total_samples += targets.size(0)\n",
    "        \n",
    "        all_targets.append(targets.cpu())\n",
    "        all_predictions.append(outputs.cpu())\n",
    "\n",
    "targets_tensor = torch.cat(all_targets)\n",
    "predictions_tensor = torch.cat(all_predictions)\n",
    "\n",
    "mse = total_loss / total_samples\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "mae_loss = nn.L1Loss()\n",
    "mae = mae_loss(predictions_tensor, targets_tensor).item()\n",
    "\n",
    "# Three vectors are currently being treated as a single output for R calculation\n",
    "# Perhaps should use per-dimension R instead\n",
    "\n",
    "# R-squared\n",
    "target_mean = targets_tensor.mean()\n",
    "ss_total = ((targets_tensor - target_mean) ** 2).sum().item()\n",
    "ss_residual = ((predictions_tensor - targets_tensor) ** 2).sum().item()\n",
    "r_squared = 1 - (ss_residual / ss_total) if ss_total != 0 else 0.0\n",
    "\n",
    "print(f'\\n--- Test Set Metrics ---')\n",
    "print(f'MSE:  {mse:.6f}')\n",
    "print(f'RMSE: {rmse:.6f}')\n",
    "print(f'MAE:  {mae:.6f}')\n",
    "print(f'R:   {r_squared:.4f}')\n",
    "print(\"\\nTraining and Evaluation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2b6c49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "69ebc215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.2384, 45.0226, 45.1613]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model(torch.tensor([[802.0]], dtype=torch.float32)))  # Example bending stiffness input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd40e773",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".cmecEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
