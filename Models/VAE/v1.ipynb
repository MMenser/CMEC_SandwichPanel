{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d5afd3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "888d329c",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 3 # thickness, height, angle\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 32 # \n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d8cfc3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 685 duplicate row(s) from the dataset.\n",
      "Shape of dataset after removing duplicates: (743, 4)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r'../MLP/processed_bending_stiffness.csv')\n",
    "\n",
    "# Remove duplicates\n",
    "initial_count = len(df)\n",
    "df = df.drop_duplicates()\n",
    "removed_count = initial_count - len(df)\n",
    "if removed_count > 0:\n",
    "    print(f\"Removed {removed_count} duplicate row(s) from the dataset.\")\n",
    "print(f\"Shape of dataset after removing duplicates: {df.shape}\")\n",
    "X = df[['Thickness', 'Height', 'Angle (deg)']]\n",
    "y = df['Bending_Stiffness']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X.values, \n",
    "    y.values, \n",
    "    test_size=TEST_SIZE, \n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "DATASET_SIZE = len(df) # Number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "084b476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all parameters have mean 0 and std 1 so parameters with a large scale don't skew \n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1))  # Add .reshape(-1, 1)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1))  # Add .reshape(-1, 1)\n",
    "\n",
    "# Create DataLoaders\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "y_train_tensor = torch.FloatTensor(y_train_scaled)  # Now has shape [N, 1]\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0699a03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self, input_dim=3, condition_dim=1, hidden_dim=64, latent_dim=8, device=device):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "\n",
    "        # encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim + condition_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2)\n",
    "            )\n",
    "        \n",
    "        # latent space - mean and variance \n",
    "        self.mean_layer = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.logvar_layer = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        # decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + condition_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "            )\n",
    "     \n",
    "    def encode(self, x, condition):\n",
    "        x_with_condition = torch.cat([x, condition], dim=1)\n",
    "        x = self.encoder(x_with_condition)\n",
    "        mean, logvar = self.mean_layer(x), self.logvar_layer(x)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterization(self, mean, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        epsilon = torch.randn_like(std).to(device)      \n",
    "        z = mean + std * epsilon\n",
    "        return z\n",
    "\n",
    "    def decode(self, z, condition):\n",
    "        # Concatenate latent with condition\n",
    "        z = torch.cat([z, condition], dim=1)\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        mean, logvar = self.encode(x, condition)\n",
    "        z = self.reparameterization(mean, logvar)\n",
    "        x_hat = self.decode(z, condition)\n",
    "        return x_hat, mean, logvar\n",
    "\n",
    "def loss_function(x, x_hat, mean, log_var):\n",
    "    reproduction_loss = nn.functional.mse_loss(x_hat, x, reduction='sum')\n",
    "    KLD = - 0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "    return reproduction_loss + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2e581559",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConditionalVAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "225aa736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch 1 \tAverage Loss:  2.929524033157914\n",
      "\tEpoch 2 \tAverage Loss:  2.3913752302176223\n",
      "\tEpoch 3 \tAverage Loss:  1.9841505622221565\n",
      "\tEpoch 4 \tAverage Loss:  1.8233193869542594\n",
      "\tEpoch 5 \tAverage Loss:  1.6820411104144473\n",
      "\tEpoch 6 \tAverage Loss:  1.5978994401617082\n",
      "\tEpoch 7 \tAverage Loss:  1.5586133886266638\n",
      "\tEpoch 8 \tAverage Loss:  1.4638690659494111\n",
      "\tEpoch 9 \tAverage Loss:  1.5252472206398293\n",
      "\tEpoch 10 \tAverage Loss:  1.486820734711207\n",
      "\tEpoch 11 \tAverage Loss:  1.445204420121832\n",
      "\tEpoch 12 \tAverage Loss:  1.4652370844625864\n",
      "\tEpoch 13 \tAverage Loss:  1.4126841098772556\n",
      "\tEpoch 14 \tAverage Loss:  1.4574933292889836\n",
      "\tEpoch 15 \tAverage Loss:  1.4028789571640066\n",
      "\tEpoch 16 \tAverage Loss:  1.3806361638335667\n",
      "\tEpoch 17 \tAverage Loss:  1.4090166188249684\n",
      "\tEpoch 18 \tAverage Loss:  1.4694382638642283\n",
      "\tEpoch 19 \tAverage Loss:  1.403570698567914\n",
      "\tEpoch 20 \tAverage Loss:  1.4074486915511315\n",
      "\tEpoch 21 \tAverage Loss:  1.4406371421685524\n",
      "\tEpoch 22 \tAverage Loss:  1.3790153509840017\n",
      "\tEpoch 23 \tAverage Loss:  1.4192480928568727\n",
      "\tEpoch 24 \tAverage Loss:  1.4036199332086325\n",
      "\tEpoch 25 \tAverage Loss:  1.3584983196322766\n",
      "\tEpoch 26 \tAverage Loss:  1.3645155710804744\n",
      "\tEpoch 27 \tAverage Loss:  1.3638103947494968\n",
      "\tEpoch 28 \tAverage Loss:  1.3720524527809836\n",
      "\tEpoch 29 \tAverage Loss:  1.4014939715966632\n",
      "\tEpoch 30 \tAverage Loss:  1.376697726522632\n",
      "\tEpoch 31 \tAverage Loss:  1.3299685038300075\n",
      "\tEpoch 32 \tAverage Loss:  1.3984840727012968\n",
      "\tEpoch 33 \tAverage Loss:  1.3521248884875365\n",
      "\tEpoch 34 \tAverage Loss:  1.3136685239746915\n",
      "\tEpoch 35 \tAverage Loss:  1.3872365341443404\n",
      "\tEpoch 36 \tAverage Loss:  1.3938814542108915\n",
      "\tEpoch 37 \tAverage Loss:  1.3496668972953\n",
      "\tEpoch 38 \tAverage Loss:  1.3452363287158284\n",
      "\tEpoch 39 \tAverage Loss:  1.3000118499653106\n",
      "\tEpoch 40 \tAverage Loss:  1.3215235636290477\n",
      "\tEpoch 41 \tAverage Loss:  1.3447571378765684\n",
      "\tEpoch 42 \tAverage Loss:  1.321668104691939\n",
      "\tEpoch 43 \tAverage Loss:  1.2939670190265282\n",
      "\tEpoch 44 \tAverage Loss:  1.3650073170260548\n",
      "\tEpoch 45 \tAverage Loss:  1.2838027838504675\n",
      "\tEpoch 46 \tAverage Loss:  1.2867074189362702\n",
      "\tEpoch 47 \tAverage Loss:  1.3733318341701521\n",
      "\tEpoch 48 \tAverage Loss:  1.2948336360430477\n",
      "\tEpoch 49 \tAverage Loss:  1.3401808144668939\n",
      "\tEpoch 50 \tAverage Loss:  1.3644252899118545\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "810.4686222076416"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train(model, optimizer, epochs, device):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        overall_loss = 0\n",
    "        total_samples = 0\n",
    "        for batch_idx, (x, condition) in enumerate(train_loader):\n",
    "            x = x.to(device)\n",
    "            condition = condition.to(device)\n",
    "            current_batch_size = x.size(0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #Forward pass\n",
    "            x_hat, mean, log_var = model(x, condition)\n",
    "            loss = loss_function(x, x_hat, mean, log_var)\n",
    "            \n",
    "            overall_loss += loss.item()\n",
    "            total_samples += current_batch_size\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss = overall_loss / total_samples\n",
    "        print(\"\\tEpoch\", epoch + 1, \"\\tAverage Loss: \", (avg_loss))\n",
    "    return overall_loss\n",
    "\n",
    "train(model, optimizer, epochs=NUM_EPOCHS, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "70502997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, stiffness, x, y, samples, device='cpu'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        normalized_stiffness = scaler_y.transform([[stiffness]])\n",
    "        stiffness_tensor = torch.FloatTensor(normalized_stiffness).to(device) # Normalize \n",
    "\n",
    "        condition = stiffness_tensor.repeat(samples, 1) # Repeat for each sample\n",
    "\n",
    "        z = torch.randn(samples, model.mean_layer.out_features).to(device)\n",
    "\n",
    "        generated = model.decode(z, condition)\n",
    "\n",
    "        print(f\"Generated: {generated}\")\n",
    "\n",
    "        generated_np = generated.cpu().numpy()\n",
    "        designs = scaler_X.inverse_transform(generated_np)\n",
    "    return designs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b65e9d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: tensor([[-0.6357, -1.2914,  0.3160],\n",
      "        [-0.8601, -1.3972, -0.8480],\n",
      "        [-0.6426, -1.4498, -0.1624],\n",
      "        [-0.8935, -1.1694,  0.4484],\n",
      "        [ 0.3886, -1.5855, -1.6253],\n",
      "        [-0.0339, -1.4889, -0.7014],\n",
      "        [-0.2616, -1.5899, -2.0291],\n",
      "        [-0.7434, -1.4306, -0.6609],\n",
      "        [ 0.7711, -1.6852, -2.1581],\n",
      "        [ 1.2103, -1.7387, -2.1039]])\n",
      "Generated designs for stiffness 50.0 :\n",
      " [[ 4.8095355 28.99014   59.367508 ]\n",
      " [ 4.473256  25.567125  43.982323 ]\n",
      " [ 4.7991514 23.865282  53.044445 ]\n",
      " [ 4.423184  32.935165  61.118065 ]\n",
      " [ 6.344819  19.47206   33.708008 ]\n",
      " [ 5.711499  22.597986  45.92066  ]\n",
      " [ 5.370283  19.330229  28.371328 ]\n",
      " [ 4.6481566 24.484632  46.455994 ]\n",
      " [ 6.917976  16.246134  26.665462 ]\n",
      " [ 7.576303  14.517836  27.382212 ]]\n"
     ]
    }
   ],
   "source": [
    "test_stiffness = 50.0\n",
    "designs = generate(model, test_stiffness, scaler_X, scaler_y, samples=10, device=device)\n",
    "\n",
    "print(\"Generated designs for stiffness\", test_stiffness, \":\\n\", designs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "11da8096",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (1400529079.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[60], line 6\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(df.loc[])\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "target = 50.0\n",
    "df = pd.read_csv(r'../MLP/processed_bending_stiffness.csv')\n",
    "df = df.drop_duplicates()\n",
    "X = df[['Thickness', 'Height', 'Angle (deg)']]\n",
    "y = df['Bending_Stiffness']\n",
    "print(df.loc[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef6a060",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".cmecEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
