{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5afd3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "888d329c",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 3 # thickness, height, angle\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 32 # \n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8cfc3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 685 duplicate row(s) from the dataset.\n",
      "Shape of dataset after removing duplicates: (743, 4)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r'../MLP/processed_bending_stiffness.csv')\n",
    "\n",
    "# Remove duplicates\n",
    "initial_count = len(df)\n",
    "df = df.drop_duplicates()\n",
    "removed_count = initial_count - len(df)\n",
    "if removed_count > 0:\n",
    "    print(f\"Removed {removed_count} duplicate row(s) from the dataset.\")\n",
    "print(f\"Shape of dataset after removing duplicates: {df.shape}\")\n",
    "X = df[['Thickness', 'Height', 'Angle (deg)']]\n",
    "y = df['Bending_Stiffness']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X.values, \n",
    "    y.values, \n",
    "    test_size=TEST_SIZE, \n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "DATASET_SIZE = len(df) # Number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "084b476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all parameters have mean 0 and std 1 so parameters with a large scale don't skew \n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1))  # Add .reshape(-1, 1)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1))  # Add .reshape(-1, 1)\n",
    "\n",
    "# Create DataLoaders\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "y_train_tensor = torch.FloatTensor(y_train_scaled)  # Now has shape [N, 1]\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0699a03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self, input_dim=3, condition_dim=1, hidden_dim=16, latent_dim=3, device=device):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "\n",
    "        # encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim + condition_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2)\n",
    "            )\n",
    "        \n",
    "        # latent space - mean and variance \n",
    "        self.mean_layer = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.logvar_layer = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        # decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + condition_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "            )\n",
    "     \n",
    "    def encode(self, x, condition):\n",
    "        x_with_condition = torch.cat([x, condition], dim=1)\n",
    "        x = self.encoder(x_with_condition)\n",
    "        mean, logvar = self.mean_layer(x), self.logvar_layer(x)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterization(self, mean, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        epsilon = torch.randn_like(std).to(device)      \n",
    "        z = mean + std * epsilon\n",
    "        return z\n",
    "\n",
    "    def decode(self, z, condition):\n",
    "        # Concatenate latent with condition\n",
    "        z = torch.cat([z, condition], dim=1)\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        mean, logvar = self.encode(x, condition)\n",
    "        z = self.reparameterization(mean, logvar)\n",
    "        x_hat = self.decode(z, condition)\n",
    "        return x_hat, mean, logvar\n",
    "\n",
    "def loss_function(x, x_hat, mean, log_var):\n",
    "    reproduction_loss = nn.functional.mse_loss(x_hat, x, reduction='sum')\n",
    "    KLD = - 0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "    return reproduction_loss + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8ae4c262",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL torch.nn.modules.container.Sequential was not an allowed global by default. Please use `torch.serialization.add_safe_globals([torch.nn.modules.container.Sequential])` or the `torch.serialization.safe_globals([torch.nn.modules.container.Sequential])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m forward_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mmason\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mWork\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mCMEC_SandwichPanel\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mModels\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mMLP\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mParameters_To_Stiffness\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mfull_model.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m forward_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcustom_loss_function\u001b[39m(x, x_hat, mean, log_var, target_stiffness, forward_model):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# 1. Standard VAE Losses\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mason\\miniconda3\\envs\\.cmecEnv\\lib\\site-packages\\torch\\serialization.py:1524\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1516\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1517\u001b[0m                     opened_zipfile,\n\u001b[0;32m   1518\u001b[0m                     map_location,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1521\u001b[0m                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1522\u001b[0m                 )\n\u001b[0;32m   1523\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1524\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1526\u001b[0m             opened_zipfile,\n\u001b[0;32m   1527\u001b[0m             map_location,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1530\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1531\u001b[0m         )\n\u001b[0;32m   1532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL torch.nn.modules.container.Sequential was not an allowed global by default. Please use `torch.serialization.add_safe_globals([torch.nn.modules.container.Sequential])` or the `torch.serialization.safe_globals([torch.nn.modules.container.Sequential])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "forward_model = torch.load(\"C:\\\\Users\\\\mason\\\\Work\\\\CMEC_SandwichPanel\\\\Models\\\\MLP\\\\Parameters_To_Stiffness\\\\full_model.pth\")\n",
    "forward_model.eval()\n",
    "\n",
    "def custom_loss_function(x, x_hat, mean, log_var, target_stiffness, forward_model):\n",
    "    # 1. Standard VAE Losses\n",
    "    repro_loss = nn.functional.mse_loss(x_hat, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "    \n",
    "    print(f\"X_hat: {x_hat}\")\n",
    "    # 2. Proxy/Consistency Loss\n",
    "    # We pass the generated designs (x_hat) into the forward model\n",
    "    # Note: x_hat is already scaled, which is what the MLP expects\n",
    "    predicted_stiffness = forward_model(x_hat)\n",
    "    \n",
    "    # Compare MLP's prediction to the target stiffness given to the VAE\n",
    "    proxy_loss = nn.functional.mse_loss(predicted_stiffness, target_stiffness, reduction='sum')\n",
    "    \n",
    "    # 3. Combine with weights (Alpha)\n",
    "    # You might need to scale proxy_loss so it doesn't overpower the others\n",
    "    alpha = 1.0 \n",
    "    return repro_loss + KLD + (alpha * proxy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e581559",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConditionalVAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "225aa736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch 1 \tAverage Loss:  3.1302884419759116\n",
      "\tEpoch 2 \tAverage Loss:  2.958769197817202\n",
      "\tEpoch 3 \tAverage Loss:  2.7794972846805046\n",
      "\tEpoch 4 \tAverage Loss:  2.62457249060223\n",
      "\tEpoch 5 \tAverage Loss:  2.3704276646829214\n",
      "\tEpoch 6 \tAverage Loss:  2.179405173870048\n",
      "\tEpoch 7 \tAverage Loss:  1.996323286884963\n",
      "\tEpoch 8 \tAverage Loss:  1.8972512447472774\n",
      "\tEpoch 9 \tAverage Loss:  1.771370621241303\n",
      "\tEpoch 10 \tAverage Loss:  1.7360547704728766\n",
      "\tEpoch 11 \tAverage Loss:  1.6835201244161586\n",
      "\tEpoch 12 \tAverage Loss:  1.6754373556837088\n",
      "\tEpoch 13 \tAverage Loss:  1.6797361341791122\n",
      "\tEpoch 14 \tAverage Loss:  1.6218953887220184\n",
      "\tEpoch 15 \tAverage Loss:  1.579169026127568\n",
      "\tEpoch 16 \tAverage Loss:  1.6154609481092255\n",
      "\tEpoch 17 \tAverage Loss:  1.5685001463199706\n",
      "\tEpoch 18 \tAverage Loss:  1.5061685099746243\n",
      "\tEpoch 19 \tAverage Loss:  1.473558753427833\n",
      "\tEpoch 20 \tAverage Loss:  1.5272336439652876\n",
      "\tEpoch 21 \tAverage Loss:  1.4431223628496883\n",
      "\tEpoch 22 \tAverage Loss:  1.5003837078107327\n",
      "\tEpoch 23 \tAverage Loss:  1.500003320199472\n",
      "\tEpoch 24 \tAverage Loss:  1.4747965488369617\n",
      "\tEpoch 25 \tAverage Loss:  1.4204293003788702\n",
      "\tEpoch 26 \tAverage Loss:  1.405571921505912\n",
      "\tEpoch 27 \tAverage Loss:  1.4567715840708928\n",
      "\tEpoch 28 \tAverage Loss:  1.4179390993985264\n",
      "\tEpoch 29 \tAverage Loss:  1.3851431515883115\n",
      "\tEpoch 30 \tAverage Loss:  1.3968061325124619\n",
      "\tEpoch 31 \tAverage Loss:  1.3905164930555556\n",
      "\tEpoch 32 \tAverage Loss:  1.4025492652096732\n",
      "\tEpoch 33 \tAverage Loss:  1.4082461366749772\n",
      "\tEpoch 34 \tAverage Loss:  1.3820666303538314\n",
      "\tEpoch 35 \tAverage Loss:  1.3882840121233906\n",
      "\tEpoch 36 \tAverage Loss:  1.4283337512802998\n",
      "\tEpoch 37 \tAverage Loss:  1.3538667248555707\n",
      "\tEpoch 38 \tAverage Loss:  1.3627624126395794\n",
      "\tEpoch 39 \tAverage Loss:  1.3811260730730563\n",
      "\tEpoch 40 \tAverage Loss:  1.4096243790905885\n",
      "\tEpoch 41 \tAverage Loss:  1.4023437114677044\n",
      "\tEpoch 42 \tAverage Loss:  1.375463655902079\n",
      "\tEpoch 43 \tAverage Loss:  1.3711768898498329\n",
      "\tEpoch 44 \tAverage Loss:  1.425724825875125\n",
      "\tEpoch 45 \tAverage Loss:  1.3667787526191686\n",
      "\tEpoch 46 \tAverage Loss:  1.4077855376683501\n",
      "\tEpoch 47 \tAverage Loss:  1.3769627227526322\n",
      "\tEpoch 48 \tAverage Loss:  1.3187620310671\n",
      "\tEpoch 49 \tAverage Loss:  1.3972688796945694\n",
      "\tEpoch 50 \tAverage Loss:  1.3764500441374603\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "817.6113262176514"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train(model, optimizer, epochs, device):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        overall_loss = 0\n",
    "        total_samples = 0\n",
    "        for batch_idx, (x, condition) in enumerate(train_loader):\n",
    "            x = x.to(device)\n",
    "            condition = condition.to(device)\n",
    "            current_batch_size = x.size(0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #Forward pass\n",
    "            x_hat, mean, log_var = model(x, condition)\n",
    "            loss = loss_function(x, x_hat, mean, log_var)\n",
    "            \n",
    "            overall_loss += loss.item()\n",
    "            total_samples += current_batch_size\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss = overall_loss / total_samples\n",
    "        print(\"\\tEpoch\", epoch + 1, \"\\tAverage Loss: \", (avg_loss))\n",
    "    return overall_loss\n",
    "\n",
    "train(model, optimizer, epochs=NUM_EPOCHS, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70502997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, stiffness, x, y, samples, device='cpu'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        normalized_stiffness = scaler_y.transform([[stiffness]])\n",
    "        stiffness_tensor = torch.FloatTensor(normalized_stiffness).to(device) # Normalize \n",
    "\n",
    "        condition = stiffness_tensor.repeat(samples, 1) # Repeat for each sample\n",
    "\n",
    "        z = torch.randn(samples, model.mean_layer.out_features).to(device)\n",
    "\n",
    "        generated = model.decode(z, condition)\n",
    "\n",
    "        generated_np = generated.cpu().numpy()\n",
    "        designs = scaler_X.inverse_transform(generated_np)\n",
    "    return designs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b65e9d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Thickness: 5.22, Average Height: 76.15, Average Angle: 58.33\n"
     ]
    }
   ],
   "source": [
    "test_stiffness = 5000.0\n",
    "designs = generate(model, test_stiffness, scaler_X, scaler_y, samples=10, device=device)\n",
    "averageThickness = np.mean(designs[:, 0])\n",
    "averageHeight = np.mean(designs[:, 1])\n",
    "averageAngle = np.mean(designs[:, 2])\n",
    "print(f\"Average Thickness: {averageThickness:.2f}, Average Height: {averageHeight:.2f}, Average Angle: {averageAngle:.2f}\")\n",
    "#print(\"Generated designs for stiffness\", test_stiffness, \":\\n\", designs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11da8096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Thickness  Height  Angle (deg)  Bending_Stiffness       diff\n",
      "582       5.351  71.215         64.1        4990.091697   9.908303\n",
      "6         3.190  93.433         63.3        4970.296544  29.703456\n",
      "1244      7.490  58.942         60.1        5045.474938  45.474938\n",
      "1206      7.395  61.011         42.7        5056.055142  56.055142\n",
      "1044      6.804  61.683         57.4        4915.316049  84.683951\n"
     ]
    }
   ],
   "source": [
    "target = 5000.0\n",
    "\n",
    "# Calculate the absolute difference\n",
    "df['diff'] = (df['Bending_Stiffness'] - target).abs()\n",
    "\n",
    "# Get the 5 rows with the smallest difference\n",
    "closest_rows = df.nsmallest(5, 'diff')\n",
    "\n",
    "print(closest_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef6a060",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".cmecEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
