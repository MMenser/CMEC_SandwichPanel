{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a98b2686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98bc0382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "INPUT_SIZE = 3  # thickness, height, angle (what we want to generate)\n",
    "CONDITION_SIZE = 1  # bending_stiffness (what we condition on)\n",
    "HIDDEN_DIM = 64\n",
    "LATENT_DIM = 8\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 100\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "ALPHA = 1.0\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cd54329",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'../MLP/processed_bending_stiffness.csv')\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "X = df[['Thickness', 'Height', 'Angle (deg)']].values\n",
    "y = df['Bending_Stiffness'].values.reshape(-1, 1)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Normalize data\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "y_test_scaled = scaler_y.transform(y_test)\n",
    "\n",
    "# Ensure float\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, \n",
    "                        shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3af1dd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Conditional VAE that generates [thickness, height, angle] \n",
    "    given a target bending_stiffness value\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=3, condition_dim=1, hidden_dim=64, latent_dim=8):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "        \n",
    "        # Encoder: takes input + condition\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim + condition_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Latent space\n",
    "        self.mean_layer = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.logvar_layer = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # Decoder: takes latent + condition\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + condition_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "     \n",
    "    def encode(self, x, condition):\n",
    "        # Concatenate input with condition\n",
    "        x_cond = torch.cat([x, condition], dim=1)\n",
    "        h = self.encoder(x_cond)\n",
    "        mean = self.mean_layer(h)\n",
    "        logvar = self.logvar_layer(h)\n",
    "        return mean, logvar\n",
    "    \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "    \n",
    "    def decode(self, z, condition):\n",
    "        # Concatenate latent with condition\n",
    "        z_cond = torch.cat([z, condition], dim=1)\n",
    "        return self.decoder(z_cond)\n",
    "    \n",
    "    def forward(self, x, condition):\n",
    "        mean, logvar = self.encode(x, condition)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        x_hat = self.decode(z, condition)\n",
    "        return x_hat, mean, logvar\n",
    "\n",
    "def custom_loss_function(x, x_hat, mean, log_var, target_stiffness, forward_model, scaler_X, alpha=1.0):\n",
    "    # 1. Standard VAE losses\n",
    "    reconstruction_loss = nn.functional.mse_loss(x_hat, x, reduction='sum')\n",
    "    kld = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "    \n",
    "    # 2. Denormalize x_hat for the physics model\n",
    "    # Extract mean and std from your scikit-learn scaler\n",
    "    # Converting to tensors for torch-compatible math\n",
    "    x_mean = torch.tensor(scaler_X.mean_, device=x_hat.device).float()\n",
    "    x_std = torch.tensor(np.sqrt(scaler_X.var_), device=x_hat.device).float()\n",
    "    \n",
    "    # x_denorm = (x_hat * std) + mean\n",
    "    x_hat_denorm = x_hat * x_std + x_mean\n",
    "    \n",
    "    # 3. Proxy/Consistency Loss\n",
    "    # Now forward_model receives raw units (thickness, height, angle)\n",
    "    predicted_stiffness = forward_model(x_hat_denorm)\n",
    "    \n",
    "    # NOTE: Ensure target_stiffness is also in RAW units if forward_model outputs raw units\n",
    "    proxy_loss = nn.functional.mse_loss(predicted_stiffness, target_stiffness, reduction='sum')\n",
    "    \n",
    "    # 4. Combine losses\n",
    "    total_loss = reconstruction_loss + kld + (alpha * proxy_loss)\n",
    "    \n",
    "    loss_dict = {\n",
    "        'total': total_loss.item(),\n",
    "        'reconstruction': reconstruction_loss.item(),\n",
    "        'kld': kld.item(),\n",
    "        'proxy': proxy_loss.item()\n",
    "    }\n",
    "    \n",
    "    return total_loss, loss_dict\n",
    "\n",
    "def train_cvae(model, forward_model, optimizer, train_loader, epochs, alpha, device):\n",
    "    \"\"\"Train the conditional VAE\"\"\"\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_losses = {\n",
    "            'total': 0,\n",
    "            'reconstruction': 0,\n",
    "            'kld': 0,\n",
    "            'proxy': 0\n",
    "        }\n",
    "        total_samples = 0\n",
    "        \n",
    "        for batch_idx, (x, condition) in enumerate(train_loader):\n",
    "            x = x.to(device)\n",
    "            condition = condition.to(device)\n",
    "            current_batch_size = x.size(0)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass through VAE\n",
    "            x_hat, mean, log_var = model(x, condition)\n",
    "            \n",
    "            # Calculate custom loss\n",
    "            loss, loss_dict = custom_loss_function(x, x_hat, mean, log_var, condition, forward_model, alpha)\n",
    "            \n",
    "            # Accumulate losses\n",
    "            for key in epoch_losses:\n",
    "                epoch_losses[key] += loss_dict[key]\n",
    "            total_samples += current_batch_size\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Calculate average losses\n",
    "        for key in epoch_losses:\n",
    "            epoch_losses[key] /= total_samples\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "            print(f\"  Total Loss: {epoch_losses['total']:.6f}\")\n",
    "            print(f\"  Reconstruction: {epoch_losses['reconstruction']:.6f}\")\n",
    "            print(f\"  KLD: {epoch_losses['kld']:.6f}\")\n",
    "            print(f\"  Proxy: {epoch_losses['proxy']:.6f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def generate_designs(model, target_stiffness, scaler_X, scaler_y, \n",
    "                     n_samples=10, device='cpu'):\n",
    "    \"\"\"\n",
    "    Generate designs given a target bending stiffness\n",
    "    \n",
    "    Args:\n",
    "        model: Trained CVAE model\n",
    "        target_stiffness: Desired bending stiffness value\n",
    "        scaler_X: StandardScaler fitted on X (thickness, height, angle)\n",
    "        scaler_y: StandardScaler fitted on y (bending_stiffness)\n",
    "        n_samples: Number of design variations to generate\n",
    "        device: 'cpu' or 'cuda'\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of shape (n_samples, 3) with [thickness, height, angle]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Normalize the target stiffness\n",
    "        target_normalized = scaler_y.transform([[target_stiffness]])\n",
    "        target_tensor = torch.FloatTensor(target_normalized).to(device)\n",
    "        \n",
    "        # Repeat for n_samples\n",
    "        condition = target_tensor.repeat(n_samples, 1)\n",
    "        \n",
    "        # Sample from prior distribution\n",
    "        z = torch.randn(n_samples, model.mean_layer.out_features).to(device)\n",
    "        \n",
    "        # Generate designs\n",
    "        generated = model.decode(z, condition)\n",
    "        \n",
    "        # Convert back to numpy and denormalize\n",
    "        generated_np = generated.cpu().numpy()\n",
    "        designs = scaler_X.inverse_transform(generated_np)\n",
    "        \n",
    "    return designs\n",
    "\n",
    "def verify_generated_designs(designs, forward_model, scaler_X, scaler_y, \n",
    "                            target_stiffness, device='cpu'):\n",
    "    \"\"\"\n",
    "    Verify that generated designs actually produce the target stiffness\n",
    "    \n",
    "    Args:\n",
    "        designs: Generated designs (denormalized)\n",
    "        forward_model: Pre-trained MLP\n",
    "        scaler_X, scaler_y: Scalers for normalization\n",
    "        target_stiffness: Target stiffness value (denormalized)\n",
    "        device: 'cpu' or 'cuda'\n",
    "    \n",
    "    Returns:\n",
    "        predicted_stiffness: Array of predicted stiffness values\n",
    "    \"\"\"\n",
    "    forward_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Normalize designs\n",
    "        designs_normalized = scaler_X.transform(designs)\n",
    "        designs_tensor = torch.tensor(designs_normalized, dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Predict stiffness\n",
    "        predicted_normalized = forward_model(designs_tensor)\n",
    "        \n",
    "        # Denormalize predictions\n",
    "        predicted_stiffness = scaler_y.inverse_transform(\n",
    "            predicted_normalized.cpu().numpy()\n",
    "        )\n",
    "        \n",
    "    return predicted_stiffness.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07a4f29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mason\\AppData\\Local\\Temp\\ipykernel_6636\\1461096617.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=3, out_features=64, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Dropout(p=0.2, inplace=False)\n",
       "  (3): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (4): ReLU()\n",
       "  (5): Dropout(p=0.2, inplace=False)\n",
       "  (6): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (7): ReLU()\n",
       "  (8): Dropout(p=0.2, inplace=False)\n",
       "  (9): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_model = nn.Sequential(\n",
    "    nn.Linear(3, 64),  # INPUT_SIZE = 3\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    \n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    \n",
    "    nn.Linear(32, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    \n",
    "    nn.Linear(16, 1)\n",
    ").to(device)\n",
    "\n",
    "# Load the saved weights\n",
    "forward_model.load_state_dict(\n",
    "    torch.load(\n",
    "        r\"C:\\Users\\mason\\Work\\CMEC_SandwichPanel\\Models\\MLP\\Parameters_To_Stiffness\\model_weight.pth\",\n",
    "        map_location=device\n",
    "    )\n",
    ")\n",
    "forward_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "368f2776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Conditional VAE with Forward Model Consistency...\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'mean_'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining Conditional VAE with Forward Model Consistency...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m model = \u001b[43mtrain_cvae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mALPHA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Generate and verify designs\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 110\u001b[39m, in \u001b[36mtrain_cvae\u001b[39m\u001b[34m(model, forward_model, optimizer, train_loader, epochs, alpha, device)\u001b[39m\n\u001b[32m    107\u001b[39m x_hat, mean, log_var = model(x, condition)\n\u001b[32m    109\u001b[39m \u001b[38;5;66;03m# Calculate custom loss\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m loss, loss_dict = \u001b[43mcustom_loss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# Accumulate losses\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m epoch_losses:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mcustom_loss_function\u001b[39m\u001b[34m(x, x_hat, mean, log_var, target_stiffness, forward_model, scaler_X, alpha)\u001b[39m\n\u001b[32m     57\u001b[39m kld = -\u001b[32m0.5\u001b[39m * torch.sum(\u001b[32m1\u001b[39m + log_var - mean.pow(\u001b[32m2\u001b[39m) - log_var.exp())\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# 2. Denormalize x_hat for the physics model\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Extract mean and std from your scikit-learn scaler\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Converting to tensors for torch-compatible math\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m x_mean = torch.tensor(\u001b[43mscaler_X\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean_\u001b[49m, device=x_hat.device).float()\n\u001b[32m     63\u001b[39m x_std = torch.tensor(np.sqrt(scaler_X.var_), device=x_hat.device).float()\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# x_denorm = (x_hat * std) + mean\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'float' object has no attribute 'mean_'"
     ]
    }
   ],
   "source": [
    "model = ConditionalVAE(\n",
    "    input_dim=INPUT_SIZE,\n",
    "    condition_dim=CONDITION_SIZE,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    latent_dim=LATENT_DIM\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Train with custom loss\n",
    "print(\"\\nTraining Conditional VAE with Forward Model Consistency...\")\n",
    "print(\"=\"*60)\n",
    "model = train_cvae(model, forward_model, optimizer, train_loader, NUM_EPOCHS, ALPHA, device)\n",
    "\n",
    "# Generate and verify designs\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING AND VERIFYING DESIGNS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "target_stiffness = 500.0  # Example target\n",
    "n_samples = 10\n",
    "\n",
    "designs = generate_designs(\n",
    "    model, \n",
    "    target_stiffness, \n",
    "    scaler_X, \n",
    "    scaler_y, \n",
    "    n_samples=n_samples, \n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Verify designs using forward model\n",
    "predicted_stiffness = verify_generated_designs(\n",
    "    designs, forward_model, scaler_X, scaler_y, \n",
    "    target_stiffness, device\n",
    ")\n",
    "\n",
    "print(f\"\\nTarget Bending Stiffness: {target_stiffness}\")\n",
    "print(\"\\nGenerated Designs and Verification:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'#':<5} {'Thickness':<12} {'Height':<12} {'Angle':<12} {'Predicted':<15} {'Error':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "errors = []\n",
    "for i, (design, pred) in enumerate(zip(designs, predicted_stiffness), 1):\n",
    "    error = abs(pred - target_stiffness)\n",
    "    errors.append(error)\n",
    "    print(f\"{i:<5} {design[0]:<12.4f} {design[1]:<12.4f} {design[2]:<12.4f} \"\n",
    "          f\"{pred:<15.2f} {error:<10.2f}\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(f\"Mean Absolute Error: {np.mean(errors):.2f}\")\n",
    "print(f\"Std Dev of Error: {np.std(errors):.2f}\")\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af811bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmecVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
