{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc674ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from datetime import datetime\n",
    "import keras\n",
    "from keras import layers, ops\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "dataset_repetitions = 5\n",
    "num_epochs = 50\n",
    "image_size = 128\n",
    "batch_size = 64\n",
    "latent_dim = 100\n",
    "\n",
    "# WGAN specific\n",
    "critic_iterations = 5\n",
    "gp_lambda = 10.0\n",
    "learning_rate = 1e-4\n",
    "\n",
    "\n",
    "def preprocess_image(data):\n",
    "    height = ops.shape(data[\"image\"])[0]\n",
    "    width = ops.shape(data[\"image\"])[1]\n",
    "    crop_size = ops.minimum(height, width)\n",
    "\n",
    "    image = tf.image.crop_to_bounding_box(\n",
    "        data[\"image\"],\n",
    "        (height - crop_size) // 2,\n",
    "        (width - crop_size) // 2,\n",
    "        crop_size,\n",
    "        crop_size,\n",
    "    )\n",
    "    image = tf.image.resize(image, size=[image_size, image_size], antialias=True)\n",
    "    return ops.clip(image / 255.0, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def prepare_dataset(split):\n",
    "    return (\n",
    "        tfds.load(dataset_name, split=split, shuffle_files=True)\n",
    "        .map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .cache()\n",
    "        .repeat(dataset_repetitions)\n",
    "        .shuffle(10 * batch_size)\n",
    "        .batch(batch_size, drop_remainder=True)\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "def load_custom_image(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_image(img, channels=1)  # 1 = grayscale\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)  # [0,1]\n",
    "\n",
    "    # Ensure shape is fully defined\n",
    "    img = tf.image.resize(img, [128, 128], antialias=True)\n",
    "\n",
    "    # Convert grayscale to 3-channel if needed\n",
    "    img = tf.image.grayscale_to_rgb(img)  # shape becomes (128,128,3)\n",
    "\n",
    "    return img\n",
    "\n",
    "def make_image_dataset(folder, batch_size=64):\n",
    "    files = tf.data.Dataset.list_files(folder + \"/*\", shuffle=True)\n",
    "\n",
    "    ds = files.map(load_custom_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.shuffle(1000)\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return ds\n",
    "\n",
    "image_folder = \"/Images/Processed_128x128_Grayscale\"\n",
    "train_dataset = make_image_dataset(image_folder, batch_size=batch_size)\n",
    "\n",
    "for img_batch in train_dataset.take(1):\n",
    "    print(img_batch.shape)  # Expected: (batch_size, 128, 128, 3)\n",
    "    plt.imshow(img_batch[0])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize sample\n",
    "for image in train_dataset.take(1):\n",
    "    plt.imshow(image[0].numpy())\n",
    "    plt.show()\n",
    "    break\n",
    "\n",
    "\n",
    "def build_generator(latent_dim, image_size):\n",
    "    return keras.Sequential([\n",
    "        keras.Input(shape=(latent_dim,)),\n",
    "        layers.Dense(4 * 4 * 256, use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(0.2),\n",
    "        layers.Reshape((4, 4, 256)),\n",
    "\n",
    "        layers.Conv2DTranspose(128, 4, 2, padding=\"same\", use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(0.2),\n",
    "\n",
    "        layers.Conv2DTranspose(64, 4, 2, padding=\"same\", use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(0.2),\n",
    "\n",
    "        layers.Conv2DTranspose(32, 4, 2, padding=\"same\", use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(0.2),\n",
    "\n",
    "        layers.Conv2DTranspose(3, 4, 2, padding=\"same\", use_bias=False),\n",
    "        layers.Activation(\"tanh\"),\n",
    "    ])\n",
    "\n",
    "\n",
    "def build_critic(image_size):\n",
    "    L = layers\n",
    "    inp = L.Input(shape=(image_size, image_size, 3))\n",
    "    nf = 64\n",
    "\n",
    "    x = L.Conv2D(nf, 4, 2, padding=\"same\")(inp)\n",
    "    x = L.LeakyReLU(0.2)(x)\n",
    "\n",
    "    x = L.Conv2D(nf * 2, 4, 2, padding=\"same\")(x)\n",
    "    x = L.LeakyReLU(0.2)(x)\n",
    "\n",
    "    x = L.Conv2D(nf * 4, 4, 2, padding=\"same\")(x)\n",
    "    x = L.LeakyReLU(0.2)(x)\n",
    "\n",
    "    x = L.Conv2D(nf * 8, 4, 2, padding=\"same\")(x)\n",
    "    x = L.LeakyReLU(0.2)(x)\n",
    "\n",
    "    x = L.Flatten()(x)\n",
    "    out = L.Dense(1)(x)\n",
    "\n",
    "    return keras.Model(inp, out)\n",
    "\n",
    "\n",
    "def gradient_penalty(critic, real, fake):\n",
    "    batch = tf.shape(real)[0]\n",
    "    eps = tf.random.uniform([batch, 1, 1, 1], 0.0, 1.0)\n",
    "    x_hat = eps * real + (1 - eps) * fake\n",
    "\n",
    "    with tf.GradientTape() as gp_tape:\n",
    "        gp_tape.watch(x_hat)\n",
    "        pred = critic(x_hat, training=True)\n",
    "\n",
    "    grads = gp_tape.gradient(pred, x_hat)\n",
    "    grads = tf.reshape(grads, [batch, -1])\n",
    "    grad_norm = tf.norm(grads, axis=1)\n",
    "    return tf.reduce_mean((grad_norm - 1.0) ** 2)\n",
    "\n",
    "\n",
    "def update_ema(G_src, G_tgt, decay=0.999):\n",
    "    src_w = G_src.get_weights()\n",
    "    tgt_w = G_tgt.get_weights()\n",
    "    new_w = [\n",
    "        decay * w_ema + (1.0 - decay) * w\n",
    "        for w_ema, w in zip(tgt_w, src_w)\n",
    "    ]\n",
    "    G_tgt.set_weights(new_w)\n",
    "\n",
    "\n",
    "generator = build_generator(latent_dim, image_size)\n",
    "critic = build_critic(image_size)\n",
    "\n",
    "G_ema = keras.models.clone_model(generator)\n",
    "G_ema.set_weights(generator.get_weights())\n",
    "\n",
    "generator_optimizer = keras.optimizers.Adam(learning_rate, 0.0, 0.9)\n",
    "critic_optimizer = keras.optimizers.Adam(learning_rate, 0.0, 0.9)\n",
    "\n",
    "print(\"Generator:\")\n",
    "generator.summary()\n",
    "print(\"\\nCritic:\")\n",
    "critic.summary()\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(real_images):\n",
    "    batch = tf.shape(real_images)[0]\n",
    "    real_images = real_images * 2.0 - 1.0\n",
    "\n",
    "    # ----- Critic -----\n",
    "    for _ in range(critic_iterations):\n",
    "        noise = tf.random.normal((batch, latent_dim))\n",
    "        fake = generator(noise, training=True)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            real_scores = critic(real_images, training=True)\n",
    "            fake_scores = critic(fake, training=True)\n",
    "\n",
    "            wasserstein = tf.reduce_mean(real_scores) - tf.reduce_mean(fake_scores)\n",
    "            gp = gradient_penalty(critic, real_images, fake)\n",
    "            critic_loss = -(wasserstein) + gp_lambda * gp\n",
    "\n",
    "        grads = tape.gradient(critic_loss, critic.trainable_weights)\n",
    "        critic_optimizer.apply_gradients(zip(grads, critic.trainable_weights))\n",
    "\n",
    "    # ----- Generator -----\n",
    "    noise = tf.random.normal((batch, latent_dim))\n",
    "    with tf.GradientTape() as tape:\n",
    "        fake = generator(noise, training=True)\n",
    "        fake_scores = critic(fake, training=True)\n",
    "        generator_loss = -tf.reduce_mean(fake_scores)\n",
    "\n",
    "    g_grads = tape.gradient(generator_loss, generator.trainable_weights)\n",
    "    generator_optimizer.apply_gradients(zip(g_grads, generator.trainable_weights))\n",
    "\n",
    "    return critic_loss, generator_loss, wasserstein, gp\n",
    "\n",
    "\n",
    "def generate_images(generator, epoch, num=16):\n",
    "    noise = tf.random.normal((num, latent_dim))\n",
    "    imgs = generator(noise, training=False)\n",
    "    imgs = (imgs + 1) / 2.0\n",
    "    imgs = tf.clip_by_value(imgs, 0.0, 1.0)\n",
    "\n",
    "    os.makedirs(\"generated_images\", exist_ok=True)\n",
    "\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(imgs[i].numpy())\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    fname = f\"generated_images/epoch_{epoch:03d}.png\"\n",
    "    plt.savefig(fname)\n",
    "    plt.close()\n",
    "    print(f\"Saved {fname}\")\n",
    "\n",
    "\n",
    "# ------------ TRAINING LOOP (NO KID) ------------\n",
    "print(\"\\nStarting training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    d_losses, g_losses = [], []\n",
    "\n",
    "    for step, batch_images in enumerate(train_dataset):\n",
    "        d_loss, g_loss, W, gp_val = train_step(batch_images)\n",
    "        update_ema(generator, G_ema, 0.999)\n",
    "\n",
    "        d_losses.append(float(d_loss))\n",
    "        g_losses.append(float(g_loss))\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(f\"  Step {step}: D={d_loss:.4f} G={g_loss:.4f} W={W:.4f} GP={gp_val:.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1} â€” Avg D={np.mean(d_losses):.4f}, Avg G={np.mean(g_losses):.4f}\")\n",
    "    generate_images(G_ema, epoch+1)\n",
    "\n",
    "print(\"\\nSaving models...\")\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "generator.save_weights(\"checkpoints/wgan_generator.h5\")\n",
    "critic.save_weights(\"checkpoints/wgan_critic.h5\")\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
